---
title: "A simulation study of trend detection methods for Integrated Ecosystem Assessment"
author: "Sean Hardison, Charles Perretti, Andy Beet, Geret DePiper"
date: "June 29, 2018"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
indent: yes
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{float}
- \usepackage[bottom]{footmisc}
- \usepackage{amsmath}
bibliography: SOE simulations.bib
---

```{r setup, include=FALSE}
# rmarkdown::render("SOE_simulations_methods.Rmd", "all") # for both pdf and html

data.dir <- "data.dir/"
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	fig.pos = 'H'
)

# list of all packages required
packages <- c("stringi","boot","tinytex","Kendall","zoo","zyp",
              "trend","dplyr","AICcmodavg","nlme",
              "gtools","tidyr","stringr","ggplot2",
              "data.table","scales","RColorBrewer",
              "colorspace","mccr","cowplot")

installLoadPackages <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE,repos='http://cran.us.r-project.org')
    sapply(pkg, require, character.only = TRUE)
}

installLoadPackages(packages)

```

<!--Function-->
```{r functions, echo = F}
#GLS-MS 
fit_lm <- function(dat, ar, m, ARsd, trend, spec = FALSE){
    dat <- dat %>% dplyr::filter(complete.cases(.))
    
    # Constant model (null model used to calculate 
    # overall p-value)
    constant_norm <-nlme::gls(series ~ 1, data = dat)
    
    #spec parameter specifies whether arima.sim incorporated AR(1) error process. When there is no 
    #AR error in the time series, we switch the GLS models to all rely on a normal error generating process.
    if (!spec){
      constant_ar1 <-
        try(nlme::gls(series ~ 1,
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(constant_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
      }
      
    } else {
      constant_ar1 <-
        try(nlme::gls(series ~ 1,
                      data = dat))
      if (class(constant_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
      }
    }
    # Linear model with normal error
    linear_norm <- nlme::gls(series ~ time, data = dat)
    
    # Linear model with AR1 error
    if (!spec){
      linear_ar1 <- 
        try(nlme::gls(series ~ time, 
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(linear_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
        }
      } else {
        linear_ar1 <- 
          try(nlme::gls(series ~ time, 
                        data = dat))
        if (class(linear_ar1) == "try-error"){
          return(best_lm <- data.frame(model = NA,
                                       aicc  = NA,
                                       coefs..Intercept = NA,
                                       coefs.time = NA,
                                       coefs.time2 = NA,
                                       pval = NA))
      }
    }
    linear_phi <- linear_ar1$modelStruct$corStruct
    linear_phi <-coef(linear_phi, unconstrained = FALSE)
    
    # Polynomial model with normal error
    dat$time2 <- dat$time^2
    poly_norm <- nlme::gls(series ~ time + time2, data = dat)

    # Polynomial model with AR1 error
    if (!spec){
      poly_ar1 <-
        try(nlme::gls(series ~ time + time2,
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(poly_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))

      }
    }else {
      poly_ar1 <-
        try(nlme::gls(series ~ time + time2,
                      data = dat))
      if (class(poly_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))

      }
    }
    poly_phi <- poly_ar1$modelStruct$corStruct
    poly_phi <- coef(poly_phi, unconstrained = FALSE)
    
    # Calculate AICs for all models
    df_aicc <-
      data.frame(model = c("poly_norm",
                           "poly_ar1",
                           "linear_norm",
                           "linear_ar1"),
                 aicc  = c(AICc(poly_norm),
                           AICc(poly_ar1),
                           AICc(linear_norm),
                           AICc(linear_ar1)),
                 coefs = rbind(coef(poly_norm),
                               coef(poly_ar1),
                               c(coef(linear_norm), NA),
                               c(coef(linear_ar1),  NA)),
                 phi = c(0, 
                         poly_phi,
                         0,
                         linear_phi),
                 # Calculate overall signifiance (need to use
                 # ML not REML for this)
                 pval = c(anova(update(constant_norm, method = "ML"),
                               update(poly_norm, method = "ML"))$`p-value`[2],
                          anova(update(constant_ar1, method = "ML"),
                                update(poly_ar1, method = "ML"))$`p-value`[2],
                          anova(update(constant_norm, method = "ML"),
                                update(linear_norm, method = "ML"))$`p-value`[2],
                          anova(update(constant_ar1, method = "ML"),
                                update(linear_ar1, method = "ML"))$`p-value`[2]))
    
    best_lm <-
      df_aicc %>%
      dplyr::filter(aicc == min(aicc))
    if (nrow(best_lm) >1){
      best_lm <- best_lm[1,]
    }
    phi <- best_lm$phi

    
    
  if (best_lm$model == "poly_norm") {
    model <- poly_norm
  } else if (best_lm$model == "poly_ar1") {
    model <- poly_ar1
  } else if (best_lm$model == "linear_norm") {
    model <- linear_norm
  } else if (best_lm$model == "linear_ar1") {
    model <- linear_ar1
  }


  return(list(best_lm = best_lm, 
              model = model))
}

#Label facets
label <- function(variable,value){
  return(facet_names[value])
}

#Confusion matrices
conf_mat <- function(df, test, filt){
  df <- df %>% filter(series.length == filt)
  
  #True positives
  true_pos_frac <- nrow(df[df$series.length != "no trend" &
                          df$p <= 0.05 &
                          df$test == test,])
  
  true_pos_tot <- nrow(df[df$trend != "no trend" &
                            df$test == test,])
  
  true_pos_freq <- true_pos_frac/true_pos_tot 
  
  #False positives
 
  false_pos_frac <- nrow(df[df$trend == "no trend" &
                        df$p <= 0.05 &
                        df$test == test,])
  
  false_pos_tot <- nrow(df[df$trend == "no trend" &
                             df$test == test,])
  
  false_pos_freq <- false_pos_frac/false_pos_tot
  
  #False negatives

  false_neg_frac <- nrow(df[df$trend != "no trend" &
                         df$p >= 0.05 &
                         df$test == test,])
  
  false_neg_tot <- nrow(df[df$trend != "no trend "&
                             df$test == test,])
  
  false_neg_freq <- false_neg_frac/false_neg_tot
  
  #true
  
  true_neg_frac <- nrow(df[df$trend == "no trend" &
                         df$p >= 0.05 &
                         df$test == test,])

  true_neg_tot <- nrow(df[df$trend == "no trend" &
                            df$test == test,])
  
  true_neg_freq <- true_neg_frac/true_neg_tot

  
  conf_mat <- data.frame(x = c("actual no","actual yes","actual no","actual yes"),
                         y = c('predicted no','predicted yes','predicted yes','predicted no'),
                         val = c(round(true_neg_freq,3), round(true_pos_freq,3),
                                 round(false_pos_freq,3),round(false_neg_freq,3)))
  
  return(conf_mat)
}

#Color palette
pal <- colorRampPalette(c("white","darkred"))
```

```{r constants for simulations, echo = F}

#set seed
set.seed(123)


run <- F #Run simulations or pull from previous results
n = 1000 #number of simulations
ARsd <- .54^.5 #standard deviation of innovations

#AR strengths
NOAR <- list()
medAR <- 0.433
strongAR <- 0.8

```

##Abstract
The identification of trends in ecosystem indicators has become a core component of ecosystem approaches to resource management, although oftentimes assumptions of statistical models are not properly accounted for in the reporting process. To explore the limitations of trend analysis of short times series, we applied three common methods of trend detection, including a generalized least squares model selection approach, the Mann-Kendall test, and Mann-Kendall test with trend-free pre-whitening to simulated time series of varying trend and autocorrelation strengths. Our results suggest that the ability to detect trends in time series is hampered by the influence of autocorrelated residuals in short series lengths. While it is known that tests designed to account for autocorrelation will approach nominal rejection rates as series lengths increase, the results of this study indicate biased rejection rates in the presence of even weak autocorrelation for series lengths often encountered in indicators developed for ecosystem-level reporting (N = 10, 20, 30). This work has broad implications for ecosystem-level reporting, where indicator time series are often limited in length, maintain a variety of error structures, and typically are assessed using a single statistical method applied uniformly across all time series. If a hypothesis testing approach for indicator trend analysis is to be implemented, we suggest first characterizing candidate series based on suitability (e.g. based on variance, autocorrelation, and series length) rather than a uniform application of tests for trend. A parametric approach to trend assessment could then be used to provide estimates of uncertainty and trend strengths from probability distributions.

##Introduction

  The development and analysis of indicators plays a key strategic role in implementing the Ecosystem Approach for a host of science, management, and intergovernmental organizations [e.g. @NOAA2006; @ICES2013; @SecretariatoftheConventiononBiologicalDiversity2004; @Pices2010; @Garcia2003; @Levin2009a]. At least partially in support of this, substantial effort has been invested in assessing indicator status and trends for the purpose of ecosystem reporting, in all of its guises [e.g. @Garfield2016; @NEFSC2018; @NEFSC2018a; @Blanchard2010; O'Brien 2017; @Butchart2010]. 

  Ecosystem-level indicators often vary greatly with respect to the length of the series under investigation. The ultimate goal of providing integrated advice often leads analysts to truncate longer datasets; generating a consistent series length across indicators for comparison purposes [e.g. @Blanchard2010; @Shin2010; @Shannon2010; @Canales2015]. Further reinforcing this approach is the fact that managers tend to focus on short-term issues [@SecretariatoftheConventiononBiologicalDiversity2004; @Wagner2013], which ultimately necessitates the assessment of trajectories at relatively short time scales.  

  These issues can lead to the use of short time series for the purpose of ecosystem reporting; i.e. less than 20 data points per indicator [@Blanchard2010; @Shin2010; @Shannon2010; @Canales2015; @Mackas2001; @Nicholson2004]. Statistical trend analysis of indicator data is the gold standard for managers, stakeholders, and analysts. However, in reality trend analysis in this context can be extremely difficult. Evidence indicates that the statistical power to identify trends using short time series may be limited in general [@Nicholson2004; @Wagner2013]. The hydrological and climatological literature shows that autocorrelation in time series can falsely inflate trend detection rates when models are incorrectly specified assuming the independence of error terms [@Kulkarni1995; @VonStorch1999a; @Zhang2000; @Wang2001a; @Yue2002a; @Bayazit2015].The magnitude of assigned trends can also be inflated by the presence of autocorrelation, and both of these problems are amplified by short time series [@Kulkarni1995; @Yue2002a]. Despite this, there has been no systematic investigation for the performance of models in detecting trends across the full breadth of indicators utilized in ecosystem reporting.

  In this manuscript we abstract away from issues surrounding the identification and vetting of appropriate indicators, but note that this in itself can be a challenging undertaking for which @Bundy2017 present a survey of the literature. We focus, instead, on the ability to statistically identify trends for the broad array of indicators used in marine ecosystem reporting; ranging from large-scale climatological and oceanographic drivers through the benefits derived by human society. We use Monte Carlo simulations to assess the performance of the most commonly applied statistical models under a range of time series lengths, trend strengths, and autocorrelation regimes. The simulations are parameterized using the properties of indicators currently presented in the Mid-Atlantic and New England State of the Ecosystem Reports, which are annual ecosystem status reports tailored for the U.S. Mid-Atlantic and New England Fishery Management Councils respectively [@NEFSC2018; @NEFSC2018a]. 
  
  Results indicate that correctly identifying trends is problematic using less than 30 data points, with both Type I and Type II error common. Even under the strongest signal-noise ratio (i.e. strong trends and no autocorrelation) tests perform poorly across all series length. The simulations highlight problems associated with standardizing approaches across indicators, and suggest that further thought is warranted on status and trend analysis in the context of ecosystem reporting.

## Methods
#### Simulations
Simulated time series were generated through the addition of $AR(1)$ autoregressive processes to first-order linear models:

\begin{equation}
\begin{split}
Y_{t} = \alpha_{0} + \alpha_{1}X_{t} + \varepsilon_{t} \\
\varepsilon_{t} = \rho\varepsilon_{t-1} + \omega_{t} \\
\omega_{t} \sim N(0, \sigma^{2})
\end{split}
 \label{eqn:1}
\end{equation}

where $Y_{t}$ is the simulated series at time $t$, $X_{t}$ is the time step, $\alpha_{1}$ is the slope component, and $\varepsilon_{t}$ is the $AR1$ error process; the strength of which is given by $\rho$, with the error component $\omega_{t}$ assumed to be derived from Gaussian white noise. The levels of $\alpha_{1}$ in our study were 0.026, .051, and .147, which we combined with three levels of $\rho$: 0, .43, and .8. These levels were chosen based on a preliminary analysis characterizing the distribution of trend and autocorrelation strengths across 2017 State of the Ecosystem time series data. Time series used in this preliminary step were normalized by $(y-\bar{y})/y_{sd}$ before parameter identification. 1000 simulations were implemented for all combinations of trend and autocorrelation strength. To test the null hypothesis of no trend in simulated time series, we used a generalized least squares model selection process, Mann Kendall test, and Mann Kendall test with trend-free pre-whitening. 

#### Generalized least squares

A generalized least squares (GLS) model selection procedure was implemented to test for trend in simulated series. Two first order linear and two quadratic GLS models were fit to each simulated time series and best models were chosen using AICc. Specifically, the models were 1) linear trend with uncorrelated residuals, 2) linear trend with correlated residuals, 3) quadratic trend with uncorrelated residuals, and 4) quadratic trend with correlated residuals. Component GLS models were derived from

\begin{equation}
\begin{split}
Y_{t} = \alpha_{0} + \alpha_{1}X_{t} + \alpha_{1}X_{t}^{2} + \varepsilon_{t} \\
\varepsilon_{t} = \rho\varepsilon_{t-1} + \omega_{t} \\
\omega_{t} \sim N(0, \sigma^{2})
\end{split}
 \label{eqn:2}
\end{equation}


The above model follows the same notation as our simulated series, with $Y_{t}$ equivalent to an observation at time $t$. Setting $\alpha_{2} = 0$ yielded linear trend models, and $\rho = 0$ gave models with uncorrelated residuals. 

#### Mann Kendall test
Further tests for trend in simulated time series were performed using the Mann-Kendall test (MK) (Mann 1945; Kendall 1975) and the more robust Mann-Kendall test with trend-free pre-whitening (MK-TFPW) [@Yue2002b]. The MK test is a nonparametric test for trend that assumes sample data are independent and identically distributed. Serial correlation within sample data will lead to inflated rejection rates of the null hypothesis of no trend if no correction steps are applied to the MK test [@Kulkarni1995], such as residual pre-whitening, although pre-whitening is known to reduce the magnitude of existing trend [@Yue2002a]. The Mann-Kendall with trend-free pre-whitening is a step-wise procedure developed by @Yue2002a to address issues introduced by pre-whitening, and is further detailed below. Under both MK and MK-TFPW frameworks, Kendall's tau statistic is given by:

\begin{equation}
\begin{split}
S = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\textrm{sgn}(Y_{j} - Y_{i}), \\
\end{split}
 \label{eqn:3}
\end{equation}

where $Y$ is the response vector, $n$ is the length of the series, and

\begin{equation}
\begin{split}
\textrm{sgn}(x)=\begin{Bmatrix}
1 & \textrm{if}\:x>0\\ 
0 & \textrm{if}\:x=0\\ 
-1 & \textrm{if}\:x<0
\end{Bmatrix}. \\
\end{split}
 \label{eqn:4}
\end{equation}


When there are no ties in the data, the variance of $S$ is given by 

\begin{equation}
\begin{split}
V(S) = \frac{n(n-1)(2n+5)}{18}, \\
\end{split}
 \label{eqn:5}
\end{equation}

and the distribution of $S$ when $n \geq 8$ is approximately normal and symmetric about a mean of 0. We then perform a two-sided test for trend using the $Z$ statistic given by



\begin{equation}
\begin{split}
\textrm{Z}=\begin{Bmatrix}
\frac{S-1}{\sqrt{V(S)}} & S>0\\ 
0 & S=0\\ 
\frac{S+1}{\sqrt{V(S)}} & S<0\\ 
\end{Bmatrix}, 
\end{split}
 \label{eqn:6}
\end{equation}

which is drawn from a normal distribution with mean of zero and variance of one [@Wang2001a]. Following @Yue2002a, a $P$ value for the test is determined using the standard normal cumulative distribution function


\begin{equation}
\begin{split}
P = \frac{1}{\sqrt{2\pi}} \int_{\infty}^{Z}e^{-t^{2}/2}\mathrm{d}t.
\end{split}
 \label{eqn:7}
\end{equation}

For significance level $\alpha$, if $Z_{a/2} > |P|$, then we reject $H_{0}$ of no trend.  


#### Mann-Kendall trend-free pre-whitening
The Mann Kendall trend-free pre-whitening procedure as developed by @Yue2002a is composed of four steps:


\begin{enumerate}
\item \textit{Removal of trend} - The slope of trend $b$ is estimated using the Theil-Sen estimator (Theil 1950a-c; Sen 1968) and removed from sample data if different from zero, where $b$ is given by

\begin{equation}
\begin{split}
b = \mathrm{Median}\left (\frac{y_{j} - y_{i}}{j - l}\right)\forall l < j .
\end{split}
 \label{eqn:8}
\end{equation}

Trend $b$ is removed from the series by

\begin{equation}
\begin{split}
y_{t}^{'} = y_{t} - bt,
\end{split}
 \label{eqn:9}
\end{equation}

where $y_{t}$ is the original series at time step $t$.
\item \textit{Trend-free pre-whitening} - A pre-whitening step is applied to the detrended series to remove the $AR(1)$ component. First, the lag-$1$ autocorrelation coefficient $\rho_{1}$ is computed using 

\begin{equation}
\begin{split}
\rho_{k} = \frac{  \frac{1}{n-k} \sum_{t=1}^{n-k}[y_{t} - E(y_{t})][y_{t+k} - E(y_{t})]}  {\frac{1}{n}\sum_{t=1}^{n}[y_{t} - E(y_{t})]^2},
\end{split}
 \label{eqn:10}
\end{equation}

where $E(y_{t})$ is the mean of the series and $\rho_{k}$ is the lag-$k$ autocorrelation coefficient. Serial correlation is then removed from the detrended series $y_{t}^{'}$ by 

\begin{equation}
\begin{split}
Y_{t}^{'} = y_{t}^{'} - \rho_{1}y_{t}^{'}.
\end{split}
 \label{eqn:11}
\end{equation}

\item \textit{Blending trend and residual series} - Trend $b$ is added to the independent residual series $Y_{t}^{'}$ by 

\begin{equation}
\begin{split}
Y_{t} = Y_{t}^{'} + bt.
\end{split}
 \label{eqn:12}
\end{equation}

\item \textit{MK test} - Trend is assessed through the application of the Mann Kendall test as discussed above.

\end{enumerate}

## Results
```{r assessing model power, echo = F}
#Initialize null data.frame

sim_results <-data.frame(matrix(ncol = 10, nrow = 0))
names(sim_results) <- c("test","series.length", "n","trend", "ar", "mae", "rmse", "p","mean_pred","mean_true")

if (run){

#Specify time series length
for (m in c(10,20,30)){
  
  notrend <- rep(0,m)
  ltrendweak <- -0.262 + (0.0255 * c(1:m)) 
  ltrendmed <- -0.262 + (0.051 * c(1:m)) 
  ltrendstrong <- -0.262 + (0.147 * c(1:m)) 
  print(paste("m =",m))
  
  #Trend strength
  for (k in c("notrend","ltrendweak","ltrendmed","ltrendstrong")){
    
    #AR strength
    for (j in c("strongAR","medAR","NOAR")){
      
      true_trend <- get(k)
      
      for (i in 1:n){
        
        print(i)
        #generate simulations
        dat <- arima.sim(list(ar = get(j)), n=m, rand.gen=rnorm, sd = ARsd)
        if (j != "NOAR"){
          dat[1] <- rnorm(1,mean=0,sd=sqrt((ARsd^2)/(1-get(j)^2)))
        }
        
        #add autocorrelated error structure to trend
        dat <- get(k) + dat
        dat <- data.frame(series = dat,
                          time = 1:length(dat))
        
        #---------------------------------GLS---------------------------------#
        #Correctly specifies model when no AR error in simulated time series
        if (j == "NOAR"){
          gls_sim <- fit_lm(dat = dat, spec = TRUE, trend = trend,
                            ar = unlist(get(k)), m = x, ARsd = ARsd)
        } else {
          gls_sim <- fit_lm(dat = dat, trend = trend,
                            ar = unlist(get(k)),m = x, ARsd = ARsd)
        }
        
        #Get prediction or assign NA if broken
        if (is.na(gls_sim[1])){
          gls_mae <- NA
          gls_rmse <- NA
          pval <- NA
          slope_pred <- NA
          slope_true <- NA
          
        } else {
          newtime <- seq(1, m, 1)
          newdata <- data.frame(time = newtime,
                                time2 = newtime^2)
          gls_pred <- AICcmodavg::predictSE(gls_sim$model,
                                            newdata = newdata,
                                            se.fit = F)
          slope_pred <- gls_pred[2] - gls_pred[1]
          
          if (slope_pred != gls_pred[3] - gls_pred[2]){
            slope_pred <- NA
          }
          
          ##Get error
          gls_rmse <- sqrt(mean((gls_pred - true_trend)^2))
          gls_mae <- mean(abs(gls_pred - true_trend))
          pval <- gls_sim$best_lm$pval
        }
        
        
        if (k == "ltrendweak"){
          slope_true <- 0.0255
        } else if (k == "notrend"){
          slope_true <- 0
        } else if (k == "ltrendmed"){
          slope_true <- 0.051
        } else if (k == "ltrendstrong"){
          slope_true <- 0.147
        }
        
        #Results DF
        gls_df <- data.frame(test = "gls",
                             series.length = m,
                             n = i,
                             trend = k,
                             ar = j,
                             mae = gls_mae,
                             rmse = gls_rmse,
                             p = pval,
                             slope_pred = slope_pred,
                             slope_true = slope_true)

        #---------------------------------MK---------------------------------#
        mk <- MannKendall(dat$series)
        mk_p <- unlist(mk[2])
        
        mk_df <- data.frame(test = "mk",
                             series.length = m,
                             n = i,
                             trend = k,
                             ar = j,
                             mae = NA,
                             rmse = NA,
                             p = mk_p,
                             slope_pred = NA,
                             slope_true = NA)
        
        #---------------------------------MK-TFPW---------------------------------#
        pw <- zyp.trend.vector(dat$series,method='yuepilon')
        
        pw_pred <- pw[[11]] + 1:m * pw[[2]]
        pw_rmse <- sqrt(mean((pw_pred - true_trend)^2))
        pw_mae <- mean(abs(pw_pred - true_trend))
        pw_p <- pw[6]
        slope_pred <- pw[[2]]
        
        pw_df <- data.frame(test = "pw",
                            series.length = m,
                            n = i,
                            trend = k,
                            ar = j,
                            mae = pw_mae,
                            rmse = pw_rmse,
                            p = pw_p,
                            slope_pred = slope_pred,
                            slope_true = slope_true)
        
        int_df <- rbind(gls_df, mk_df, pw_df)
        assign('sim_results',rbind(sim_results, int_df))
        
    } 
   }
  }
}
  save(sim_results, file = file.path(data.dir,paste0("sim_results",Sys.Date(),".Rdata")))
}
  

```

```{r simulation results processing, echo = F}

if (!run){load(file.path(data.dir,paste0("sim_results2018-08-29.Rdata")))}
if (!run){load(file.path(data.dir,paste0("sim_results_extended2018-08-29.Rdata")))}

#set factor levels for plotting
sim_results$trend <- sim_results %>% pull(trend) %>%
                    plyr::mapvalues(., from = c("notrend","ltrendweak","ltrendmed","ltrendstrong"),
                      to = c("no trend","weak trend","medium trend","strong trend")) %>%
                      as.factor()

sim_results$ar <- sim_results %>% pull(ar) %>%
  plyr::mapvalues(., from = c("NOAR","medAR","strongAR"),
                  to = c("no AR","medium AR","strong AR")) %>%
  as.factor()

sim_results$trend = factor(sim_results$trend, levels=c('strong trend','medium trend','weak trend','no trend'))
sim_results$ar = factor(sim_results$ar, levels=c('no AR','medium AR','strong AR'))
sim_results$series.length = factor(sim_results$series.length, levels=c(10,20,30))

#aggregate p values for plotting
p_agg <- sim_results %>% group_by(series.length, trend, ar, test) %>%
  dplyr::summarise(prop = length(p[p < 0.05])/n())
```

```{r power subset, echo = F}
if (run){
set.seed(100)
n <- 1000
sim_results_ext <-data.frame(matrix(ncol = 8, nrow = 0))
names(sim_results_ext) <- c("test","series.length", "n","trend", "ar", "mae", "rmse", "p")

for (m in seq(50,500,50)){
  
  notrend <- rep(0,m)
  ltrendweak <- -0.262 + (0.004 * c(1:m)) 
  ltrendmed <- -0.262 + (0.051 * c(1:m)) 
  ltrendstrong <- -0.262 + (0.147 * c(1:m)) 
  print(paste("m =",m))
  
  #Trend strength
  for (k in c("notrend")){
    
    #AR strength
    for (j in c("strongAR")){
      
      true_trend <- get(k)
      
      for (i in 1:n){
        
        print(i)
        #generate simulations
        dat <- arima.sim(list(ar = get(j)), n=m, rand.gen=rnorm, sd = ARsd)
        dat[1] <- rnorm(1,mean=0,sd=sqrt((ARsd^2)/(1-get(j)^2)))
        
        #add autocorrelated error structure to trend
        dat <- get(k) + dat
        dat <- data.frame(series = dat,
                          time = 1:length(dat))
        
        #---------------------------------GLS---------------------------------#
        #Correctly specifies model when no AR error in simulated time series
        if (j == "NOAR"){
          gls_sim <- fit_lm(dat = dat, spec = TRUE, trend = trend,
                            ar = unlist(get(k)), m = x, ARsd = ARsd)
        } else {
          gls_sim <- fit_lm(dat = dat, trend = trend,
                            ar = unlist(get(k)),m = x, ARsd = ARsd)
        }
        
        #Get prediction or assign NA if broken
        if (is.na(gls_sim[1])){
          gls_mae <- NA
          gls_rmse <- NA
        } else {
          newtime <- seq(1, m, 1)
          newdata <- data.frame(time = newtime,
                                time2 = newtime^2)
          gls_pred <- AICcmodavg::predictSE(gls_sim$model,
                                            newdata = newdata,
                                            se.fit = F)
          ##Get error
          gls_rmse <- sqrt(mean((gls_pred - true_trend)^2))
          gls_mae <- mean(abs(gls_pred - true_trend))
        }
        
        #Results DF
        gls_df <- data.frame(test = "gls",
                             series.length = m,
                             n = i,
                             trend = k,
                             ar = j,
                             mae = gls_mae,
                             rmse = gls_rmse,
                             p = gls_sim$best_lm$pval)

        #---------------------------------MK---------------------------------#
        mk <- MannKendall(dat$series)
        mk_p <- unlist(mk[2])
        
        mk_df <- data.frame(test = "mk",
                             series.length = m,
                             n = i,
                             trend = k,
                             ar = j,
                             mae = NA,
                             rmse = NA,
                             p = mk_p)
        
        #---------------------------------MK-TFPW---------------------------------#
        pw <- zyp.trend.vector(dat$series,method='yuepilon')
        
        pw_pred <- pw[[11]] + 1:m * pw[[2]]
        pw_rmse <- sqrt(mean((pw_pred - true_trend)^2))
        pw_mae <- mean(abs(pw_pred - true_trend))
        pw_p <- pw[6]
        
        pw_df <- data.frame(test = "pw",
                            series.length = m,
                            n = i,
                            trend = k,
                            ar = j,
                            mae = pw_mae,
                            rmse = pw_rmse,
                            p = pw_p)
        
        int_df <- rbind(gls_df, mk_df, pw_df)
        assign('sim_results_ext',rbind(sim_results_ext, int_df))
        
    } 
   }
  }
}
  save(sim_results_ext, file = file.path(data.dir,paste0("sim_results_extended",Sys.Date(),".Rdata")))
}
```



  Simulation results show that no test for trend performed well in all scenarios of simulated trend strength, time series length, and autocorrelation strength. As has been documented elsewhere [@Yue2002a; @Yue2002b], we show in Figure \ref{Fig1} that time series length has a large effect on the power of each test. Under no autocorrelation, tests for trend are not effective at detecting trends in series with N < 30. When N = 10 with no autocorrelation and strong trend ($\beta$ = 0.8), no test detected trend in >50% of series. The rate of failing to reject the null hypothesis in the presence of trend decreased to < 0.02 when N = 20 with no autocorrelation and strong trend. The effect of increased power with increasing series length and no autocorrelation diminished with reductions in trend strength across all tests. Under no autocorrelation, the GLS-MS test showed the highest rejection rates compared to other tests, although this effect was minimal (mean difference of <3% change in rejection rates between GLS and MK-TFPW). All tests returned rejection rates near the nominal significance level of 0.05 under the no trend and no autocorrelation scenarios, with the largest departures occurring when N = 10 ($MK-TFPW_{rej} = 0.096$, $MK_{rej}= 0.035$, $GLS_{rej}= 0.065$).
  
```{r power analysis figure, echo = F, fig.align='center',fig.cap="Barplots showing the ratio of number of rejections (\\textit{P}<0.05) to number of total simulations. Subplots are representative of different autocorrelation and trend scenarios, with time series length increasing along the x axis. Colored bars show results from different tests for trend.\\label{Fig1}"}

levels(p_agg$test) <- c("GLS","Mann-Kendall","MK-TFPW")
names(p_agg)[4] <- "Method"
ggplot(p_agg, aes(color = Method, y = prop, 
                  x = series.length)) +
  geom_bar(aes(fill = Method), stat = "identity",  position="dodge2",
            size = 0.5) +
  facet_grid(trend ~ ar, labeller = labeller(tbl)) +
  ylab("Proportion significant") +
  xlab("Series length") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))

```


  Autocorrelation is known the reduce the power of the MK test by increasing the variance of the \textit{S} statistic [@VonStorch1999a; @Yue2002a], and our work also shows that under no simulated trend, introducing autocorrelation will lead to inflated rejection rates in the MK test. Figure 1 shows that under no trend and strong autocorrelation ($\rho = 0.433$ and $\rho = 0.8$), the rejection rate of the Mann Kendall test increases with series length. All other tests showed decreases in rejection rates under both medium and strong autocorrelation scenarios with increasing series length.

  The GLS procedure performed the best under the no trend and strong autocorrelation scenario: when N = 30, the rejection rate for the GLS was 0.16, a 46% and 64% decrease from the MK-TFPW and MK tests respectively. The performance of the GLS test was also more strongly affected by sample size than the MK-TFPW test. When there was strong autocorrelation and no trend, rejection rates of the MK-TFPW test decreased only 13.9% between N = 10 and N = 30. Under the same conditions and GLS approach, rejection rates decreased by 59%. However, the GLS approach also performed the worst under no trend and strong autocorrelation when N = 10. This shows that while there was improvement between both tests as series lengths increased, neither test was effective in accounting for biases of autocorrelation when $N \leq 30$.  

Extending this no trend and strong autocorrelation scenario out to longer series lengths shows that the GLS test reaches nominal rejection rates of 0.05 only when $N > 200$ (Figure \ref{Fig2}). The same may be true of the MK-TFPW test when $N > 350$, although this work did not seek to identify a precise value of $N$ where the MK-TFPW approach reached nominal levels. As expected, the MK test performed poorly in this scenario, and saw no reduction in rejection rates as $N$ increased. 

```{r power subset figure, fig.align="center", fig.asp=0.45, eval = T, echo = F, fig.show = "h", fig.cap= "Barplot showing the ratio of number of rejections (\\textit{P}<0.05) to number of total simulations, when simulations were created under the parameters of no trend $(\\alpha_{1} = 0)$, strong autocorrelation $(\\rho = 0.8)$, and series lengths between \\textit{N} = 50 to $N = 500$. The dashed red line shows the nominal rejection rate of 0.05.\\label{Fig2}"}

pow <- sim_results_ext %>% group_by(series.length, test) %>% dplyr::summarise(rej_prop = length(p[p<0.05])/n())

pow$series.length <- as.factor(pow$series.length)
levels(pow$test) <- c("GLS","Mann-Kendall","MK-TFPW")
names(pow)[2] <- "Method"

ggplot(pow, aes(x = series.length, y = rej_prop, group = Method,fill = Method)) +
  geom_bar(stat = "identity",position="dodge") +
  geom_hline(yintercept=0.05, linetype="dashed", 
             color = "red", size=1) +
  ylab("Proportion significant") +
  xlab("Series length") +
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))
```

  Under strong autocorrelation and strong trend, there were increasing rejection rates for all tests with increasing series length. When $\alpha_{1} \leq 0.8$, the pattern of increasing rejection rate with series length transitions into a reduction in rejection rates, which show the effects of trend on the test. Under scenarios when trend exists and autocorrelation is strong, the MK-TFPW test was slightly better than the GLS approach at detecting trend, although both tests were only able to detect trend in >50% of simulations when trend was strong $(\alpha_{1} = 0.8)$. Between the no autocorrelation and strong autocorrelation scenarios, an increase in rejection rates was driven by autocorrelation. This was especially true for $N = 10$, where GLS rejection rates increased by 329% and MK-TFPW rejection rates by 252% between scenarios. When $N = 30$ and other conditions held constant, rejection rates decreased between no autocorrelation and strong autocorrelation scenarios. The relative success of each test when $N = 30$ can be seen in Figure \ref{Fig3}, which shows that the GLS approach was most effective in avoiding false positives, but performed similarly to the MK-TFPW test in terms of false negatives.    
  


```{r confusion matrices, echo = F, fig.align='center', out.extra='trim={0cm 5cm 0cm 0cm},clip', fig.cap= 'Confusion matrices showing aggregate results from testing for trend across all combinations autocorrelation and trend strength when N=30. Colors represent the performance of individual cells across tests, where cells shaded in red indicate a poorer outcome. For example, when N=30, the GLS procedure falsely predicted a trend when there was none in 11.1\\% of cases (white), whereas this was true in 22.1\\% of Mann-Kendall simulations (red).\\label{Fig3}' }

mk <- cbind(conf_mat(sim_results,test = "mk", filt = 30), test = rep('mk',4))
pw <- cbind(conf_mat(sim_results, test = "pw", filt = 30), test = rep('pw',4))
gls <- cbind(conf_mat(sim_results, test = "gls", filt = 30), test = rep('gls',4))
fin <- rbind(mk, pw, gls)
fin$group <- factor(paste(fin$x,fin$y))


#Make matrices for white = good and orange = bad
fin_dif <- fin %>% group_by(group) %>%
  mutate(val, best_dif = ifelse(group == "actual no predicted no"|
                                  group == "actual yes predicted yes",
                                (abs(max(val) - val)), #best_dif is for assigning colors
                                (abs(min(val) - val)))) 

#Facet titles
facet_names <- list(
  'mk'="Mann-Kendall",
  'pw'="MK-TFPW",
  'gls'="GLS"
)

#plot
ggplot(data = fin_dif, aes(x,y, fill = best_dif)) +
  facet_grid(. ~ test, labeller = label)+
  geom_tile(aes(size = 1),color = "grey", size = 1)  +
  scale_fill_gradientn(colors = pal(10))+
  geom_text(aes(x = x, y = y, label = round(val,3), size = 1),size = 4) +
  theme(legend.position = "none",
        axis.line = element_blank(),
        axis.title=element_blank(),
        axis.text.y = element_text(margin = margin(t = 0, r = -6,
                                                   b = 0, l = 0),
                                   size = 8),
        axis.text.x = element_text(margin = margin(t = -3, r = 0,
                                                   b = 150, l = 0),
                                   size = 8),
        axis.ticks.y=element_blank(),
        axis.ticks.x=element_blank(),
        plot.title = element_text(hjust = -0.1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
```

The distributions of assigned trend slopes to time series with known trends is shown in Figure \ref{Fig4}. In the nonparametric case, we used Sen's slope (as derived in Equation 8), which is a common complement to the MK and MK-TFPW significance tests. Few differences emerge when comparing the GLS and Sen's slope trend distributions, and the two procedures tend to provide similar estimates of trend under varying autocorrelation, trend, and series lengths. However, it should be noted that at smaller sample sizes (e.g. N = 10, 20) the distributions of assigned trend expand.    

Sen's slope and the GLS estimator perform similarly across all scenarios. For both methods, variation of the assigned trend from the known trend increased with autocorrelation strength. Under strong autocorrelation and $N = 30$, the GLS estimator tended to have slightly larger deviation of RMSE than Sen's slope. Across trend strengths when $N = 30$ and with strong autocorrelation, the standard deviation of RMSE for the GLS estimator was 13% larger than that of Sen's slope. 

```{r MCCR, echo = F, fig.align='center',fig.width = 8, fig.height= 3.25, fig.cap="The effect of autocorrelation and series length on Matthews Correlation Coefficient; a ", eval = F}
#Matthew correlation coefficient
p_resultsm <- p_results %>% mutate(`Trend strength`,
                                   actual = plyr::mapvalues(`Trend strength`,
                                                            from = c("strong trend","medium trend",
                                                                     "weak trend", "no trend"),
  to = c(1,1,1,0))) %>%
  mutate(predict = ifelse(Value < 0.05,1,0 )) %>%
  filter(!is.na(Value))

mk <- p_resultsm[p_resultsm$method == 'mk',]
pw <- p_resultsm[p_resultsm$method == 'pw',]
gls <- p_resultsm[p_resultsm$method == 'gls',]


#plotting MCCR across AR strengths
df <- p_resultsm
mccr_ar <- function(df,ar,time = NULL){
  if(!is.null(time)){
    z <- df[df$`timeseries length` == time,]
    z_mccr <- mccr(z$actual, z$predict)
    return(as.numeric(z_mccr)) 
  } else {
    z <- df[df$`AR strength` == ar,]
    z_mccr <- mccr(z$actual, z$predict)
    return(as.numeric(z_mccr))  
  }
 
}

mcc_mk <- data.frame(mcc = c(mccr_ar(mk, "no AR"),mccr_ar(mk, "medium AR"),mccr_ar(mk, "strong AR")),
                var = c("no AR","med. AR AR","strong AR"),
                Test = "Mann-Kendall",
                id = "AR Strength")
mcc_pw <- data.frame(mcc = c(mccr_ar(pw, "no AR"),mccr_ar(pw, "medium AR"),mccr_ar(pw, "strong AR")),
                var = c("no AR","med. AR","strong AR"),
                Test = "MK-TFPW",
                id = "AR Strength")
mcc_gls <- data.frame(mcc = c(mccr_ar(gls, "no AR"),mccr_ar(gls, "medium AR"),mccr_ar(gls, "strong AR")),
                 var = c("no AR","med. AR","strong AR"),
                 Test = "GLS",
                 id = "AR Strength")

mcc.ar <- rbind(mcc_mk, mcc_pw, mcc_gls)

ar <- ggplot(data = mcc.ar, aes(x = var, y = mcc, group = Test))+
  geom_line(aes(color = Test), size = 1.1) +
  geom_point(aes(color = Test), size = 1.5) +
  scale_x_discrete(limits=c("no AR","med. AR","strong AR"))+
  labs(x = "Autocorrelation strength",
       y = "MCC") +
  theme(axis.text = element_text(colour="grey20",size=11,hjust=.5,vjust=.5,face="plain"),
        axis.title.x = element_text(colour="grey20",size=17,angle=0,vjust=-1,face="plain"),
        axis.title.y = element_text(colour="grey20",size=17,face="plain"),
        legend.position = "none")

#MCCR across time series lengths

mcc_mk_time <- data.frame(mcc = c(mccr_ar(mk, time = 10),mccr_ar(mk, time = 20),mccr_ar(mk, time = 30)),
                     var = c(10,20,30),
                     Test = "Mann-Kendall",
                     id = "Series Length")
mcc_pw_time <- data.frame(mcc = c(mccr_ar(pw, time = 10),mccr_ar(pw, time = 20),mccr_ar(pw, time = 30)),
                          var = c(10,20,30),
                     Test = "MK-TFPW",
                     id = "Series Length")
mcc_gls_time <- data.frame(mcc = c(mccr_ar(gls, time = 10),mccr_ar(gls, time = 20),mccr_ar(gls, time = 30)),
                           var = c(10,20,30),
                      Test = "GLS",
                      id = "Series Length")

mcc.time <- rbind(mcc_mk_time, mcc_pw_time, mcc_gls_time)


time <- ggplot(data = mcc.time, aes(x = var, y = mcc, group = Test))+
  geom_line(aes(color = Test), size = 1.1) +
  geom_point(aes(color = Test), size = 1.5) +
  scale_x_discrete(limits=c(10,20,30))+
  labs(x = "Series length",
       y = "MCC") +
  theme(axis.text = element_text(colour="grey20",size=13,hjust=.5,vjust=.5,face="plain"),
        axis.title.x = element_text(colour="grey20",size=17,angle=0,vjust=-1,face="plain"),
        axis.title.y = element_text(colour="grey20",size=17,face="plain")) +
  xlim(5,35)

plot_grid(ar, time, align = "h", rel_widths = c(1, 1.51), labels = c("A","B"),
          label_x = c(0.225,0.15),label_fontface = 'plain')
```

```{r trend slope, fig.align="center", echo = F, fig.align = "center", fig.cap="Violin plots showing probability densities of estimated trends from GLS and Sen's slope procedures under varying autocorrelation scenarios $(\\rho = 0, 0.43, 0.8)$ and simulation lengths (N = 10, 20, 30). Black lines represent the median slope estimate, and red lines the true slope. For this exercise, the GLS model selection procedure was constrained to fit only linear models of trend.\\label{Fig4}"}

load(file.path(data.dir,"sim_results2018-09-18.Rdata"))

df <- sim_results %>% filter(!is.na(slope_pred), p < 0.05) 

#set factor levels for plotting
df$trend <- df %>% pull(trend) %>%
  plyr::mapvalues(., from = c("notrend","ltrendweak","ltrendmed","ltrendstrong"),
                  to = c("no trend","weak trend","medium trend","strong trend")) %>%
  as.factor()

df$ar <- df %>% pull(ar) %>%
  plyr::mapvalues(., from = c("NOAR","medAR","strongAR"),
                  to = c("no AR","medium AR","strong AR")) %>%
  as.factor()

df$trend = factor(df$trend, levels=c('strong trend','medium trend','weak trend','no trend'))
df$ar = factor(df$ar, levels=c('no AR','medium AR','strong AR'))
df$series.length = factor(df$series.length, levels=c(10,20,30))
names(df)[1] <- "Method"
levels(df$Method) <- c("GLS","Mann-Kendall","Sen's slope")

ggplot(df, aes(factor(series.length), slope_pred)) + 
  geom_hline(aes(yintercept = slope_true), color = "red", size = 0.5) +
  geom_violin(aes(fill = Method), adjust = 0.5, scale = "width",
              draw_quantiles = 0.5, size = 0.3) +
  facet_grid(trend ~ ar) +
  ylab("Predicted slope") +
  xlab("Series length") +
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))
```


##Discussion
Ecosystem reporting is vital to the development of Integrated Ecosystem Assessments (IEA), which lay out the framework for moving toward Ecosystem-Based Fishery Management (EBFM)[@Levin2009a]. The key analytical foundations to all IEA products revolve around the concept of indicator change; whether in the short or long-term, although oftentimes managers are most interested in short-term, abrupt changes to indicator status [@Wagner2013]. Here we addressed the shortcomings of assigning significant trends to indicator time series given the common problems of small sample size and autocorrelation. Our results show that commonly used statistical methods used to detect trend in the presence of autocorrelated residuals fail when assessing data with small sample size.

In the Northeast US, most indicators considered in the ecosystem assessment process are annual data typically ranging between 10-60 years in length. In the context of hydrological literature, the upper limit of time series lengths seen in our indicator data sets would be considered short [@Bayazit2015], and although it may be tempting to say otherwise, our work here highlights the dangers of assigning trends to short time series even while attempting to address problems of autocorrelation. The influence of autocorrelation in short series inevitably increases Type II error rates, or the failure to identify trend when it exists. This was especially true under scenarios of strong autocorrelation, which effectively masked the detection of trend unless trend was strong and N was large. An increase in Type I error, or the false rejection of the null hypothesis, occurred as the strength of autocorrelation increased. A departure from nominal rejection rates was also seen from all tests with even moderate amounts of autocorrelation.

Our work also focused on the capacity of tests to detect weak trend strengths in simulated time series. We found that under the limitations imposed by series lengths $\leq$ 30, no test was effective in detecting weak trends, even without the influence of autocorrelation. This result supports the work of others [e.g. @Wagner2013] that found small changes to trend in time series of limited length were difficult to detect regardless of autocorrelation. Our ability to detect these trends was likely hindered by the constant variance used to simulate series; however, this value (0.54) was chosen because it was the mean variance of real ecosystem indicators. 

As shown in Figure \ref{Fig2}, there is no solution in small sample sizes; however, we do not suggest there is no value in assigning trends to time series. Instead, we advise that a "shotgun" approach to assessing trends in many indicator time series without consideration of error structure and series lengths will likely lead to both Type I and Type II error. If the binary approach of hypothesis testing is to be implemented in ecosystem indicator reporting, a more hands-on approach should be implemented to determine indicators that are well-suited for trend analysis (e.g. a long series with low variance and weak autocorrelation). Further, the results of this study showed that the GLS approach to modeling trend ameliorated error rates compared to the Mann-Kendall test with trend-free pre-whitening. The parametric approach also carries the benefits of modeling trend uncertainty and coefficients based on a test statistic with assumed probability distribution.

Another approach would be to move towards a more flexible trend assessment framework. For example, @Nicholls2001a suggests that confidence intervals for trend effect size would suffice as an alternative to assigning significance. In this case, if the confidence interval were to contain a slope of 0, then the null hypothesis of no trend would be accepted. Bayesion inference frameworks may also provide a way forward. @Wagner2013 suggests Dynamic Linear Models (DLMs) for indicators of small sample size. DLMs allow for model coefficients (e.g. slope) to change with time while providing probabilities of rate changes. This approach introduces greater complexity into the common "up or down" model subscribed to by current ecosystem status reports, and could therefore provide greater insight to managers. 


Deriving trends from disparate ecosystem indicators is challenging in part due to the goal of applying a single statistical approach to time series with a wide range of series lengths and error structures. The complexity of the chosen method must be balanced with its applicability to a wide range of indicators and the interpretability of its results. Our work shows that blindly implementing this approach will likely result in assigning spurious trends or missing important patterns. However, programmatic consideration of candidate series for trend analysis would likely ameliorate some instance of error. Implementation of a parametric test for trend (e.g. the GLS procedure in our study) then has the benefit of providing estimates of uncertainty and trend based on a probability distribution. A subtler approach for trend analyses in ecosystem reporting would provide better outcomes for economic, ecological, and social systems in the context of EBFM decision-making.


<!--
Although we focused solely on trend detection in indicator time series, our results imply that the limitations inherent within short time series be taken into account during the indicator selection stage. 
--> 
 



```{r decile coverage simulations, echo = F, eval = F}

#placeholders for results
gls.ts.NOAR.notrend <- NULL
gls.ts.NOAR.ltrendweak <- NULL
gls.ts.NOAR.ltrendmed <- NULL
gls.ts.NOAR.ltrendstrong <- NULL

gls.ts.medAR.notrend <- NULL
gls.ts.medAR.ltrendweak <- NULL
gls.ts.medAR.ltrendmed <- NULL
gls.ts.medAR.ltrendstrong <- NULL

gls.ts.strongAR.notrend <- NULL
gls.ts.strongAR.ltrendweak <- NULL
gls.ts.strongAR.ltrendmed <- NULL
gls.ts.strongAR.ltrendstrong <- NULL

sim_results_10 <- NULL
sim_results_20 <- NULL
sim_results_30 <- NULL


if (run){
  ptm <- proc.time()
  #Specify time series length
  for (m in c(10,20,30)){
  
  notrend <- rep(0,m)
  ltrendweak <- -0.262 + (0.004 * c(1:m)) 
  ltrendmed <- -0.262 + (0.051 * c(1:m)) 
  ltrendstrong <- -0.262 + (0.147 * c(1:m)) 
  print(paste("m=",m))

    #Trend strength
    for (k in c("notrend","ltrendweak","ltrendmed","ltrendstrong")){
    
    #AR strength
    for (j in c("strongAR","medAR","NOAR")){
    
      true_trend <- get(k)
      
      for (i in 1:nsims){
        
        #generate simulations
        dat <- arima.sim(list(ar = get(j)), n=m, rand.gen=rnorm, sd = ARsd)
        
        #add autocorrelated error structure to trend
        dat <- get(k) + dat
        dat <- data.frame(series = dat,
                          time = 1:length(dat))
        
        #---------------------------------GLS---------------------------------#
        gls_sim <- tryCatch({
          newtime <- seq(1, m, 1)
          newdata <- data.frame(time = newtime,
                                time2 = newtime^2)
          
          #Correctly specifies model when no AR error in simulated time series
          if (j == "NOAR"){
            gls_sim <- fit_lm(dat = dat, ar = get(j),
                              ARsd = ARsd, m = m, trend = get(k), spec = TRUE)
          } else{
            gls_sim <- fit_lm(dat = dat, ar = get(j), ARsd = ARsd, m = m, trend = get(k))
            
          }
          
        }, 
        error = function(e) {
          gls_sim <- "error"
        })
        
        
        #---------------------------------Decile coverage---------------------------------#
        if (is.na(gls_sim[1]) | gls_sim[1] == "error"){
          
          gls_pred <- data.frame(fit = rep(NA, m))
          decile_of_true <- rep(NA, m)
        } else {
          gls_pred <- AICcmodavg::predictSE(gls_sim$model,
                                            newdata = newdata,
                                            se.fit = TRUE)
          decile_of_true <- ceiling(10 * pnorm(q    = true_trend, 
                                           mean = gls_pred$fit, 
                                           sd   = gls_pred$se.fit))
        }
        
        

        
        for (g in 1:m){
          if (is.na(gls_pred$fit[1])){
            print("NA")
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),NA))
          } else {
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),decile_of_true[g]))
          }
          
        }
          
        
      } 
      
    }
  }
  sim_results = data.frame(gls.NOAR.ltrendweak = gls.ts.NOAR.ltrendweak,
                           gls.NOAR.ltrendmed = gls.ts.NOAR.ltrendmed,
                           gls.NOAR.ltrendstrong = gls.ts.NOAR.ltrendstrong,
                           gls.NOAR.notrend = gls.ts.NOAR.notrend,

                           gls.medAR.ltrendweak = gls.ts.medAR.ltrendweak,
                           gls.medAR.ltrendmed = gls.ts.medAR.ltrendmed,
                           gls.medAR.ltrendstrong = gls.ts.medAR.ltrendstrong,
                           gls.medAR.notrend = gls.ts.medAR.notrend,

                           gls.strongAR.ltrendweak = gls.ts.strongAR.ltrendweak,
                           gls.strongAR.ltrendmed = gls.ts.strongAR.ltrendmed,
                           gls.strongAR.ltrendstrong = gls.ts.strongAR.ltrendstrong,
                           gls.strongAR.notrend = gls.ts.strongAR.notrend)

    assign(paste0('sim_results_',m), rbind(get(paste0('sim_results_',m)),sim_results))
    #write.csv(sim_results, file = paste0("decile_coverage_results_",m,Sys.Date(),".csv"))
    
    gls.ts.NOAR.notrend <- NULL
    gls.ts.NOAR.ltrendweak <- NULL
    gls.ts.NOAR.ltrendmed <- NULL
    gls.ts.NOAR.ltrendstrong <- NULL
    
    gls.ts.medAR.notrend <- NULL
    gls.ts.medAR.ltrendweak <- NULL
    gls.ts.medAR.ltrendmed <- NULL
    gls.ts.medAR.ltrendstrong <- NULL
    
    gls.ts.strongAR.notrend <- NULL
    gls.ts.strongAR.ltrendweak <- NULL
    gls.ts.strongAR.ltrendmed <- NULL
    gls.ts.strongAR.ltrendstrong <- NULL
    print(proc.time() - ptm)
}

coverage_10 <- sim_results_10  
coverage_20 <- sim_results_20  
coverage_30 <- sim_results_30  
  
write.csv(sim_results_10, file = paste0(data.dir,"coverage_10","_",date,".csv"))
write.csv(sim_results_20, file = paste0(data.dir,"coverage_20","_",date,".csv"))
write.csv(sim_results_30, file = paste0(data.dir,"coverage_30","_",date,".csv"))
  
} else if (!run){

  coverage_10 <- read.csv(file = paste0(data.dir,"coverage_10","_",date,".csv"))
  coverage_20 <- read.csv(file = paste0(data.dir,"coverage_20","_",date,".csv"))
  coverage_30 <- read.csv(file = paste0(data.dir,"coverage_30","_",date,".csv"))
}


```

```{r decile coverage plots, echo = F, fig.align='center', eval = F}

#---------------------------Process data--------------------------#
sim_30 <- tidyr::gather(coverage_30, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_30$`Series length` <- "30"
sim_20 <- tidyr::gather(coverage_20, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_20$`Series length` <- "20"
sim_10 <- tidyr::gather(coverage_10, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_10$`Series length` <- "10"
sims <- rbind(sim_10, sim_20, sim_30)

sim_table <- sims %>% group_by(`Series length`, var,value)  %>%
  filter(value != 0) %>%
  tally() %>%
  group_by(`Series length`) %>% mutate(n = n/(as.numeric(`Series length`)*nsims))#freq. table


#------------------Split out columns for grouping----------------#
col <- do.call(rbind.data.frame, str_split(sim_table$var, '[.]'))
names(col) <- c("Test","AR","Trend Strength")
sim_table$Test <- col$Test
sim_table$AR <- col$AR
sim_table$Trend.Strength <- col$`Trend Strength`

#Set factor levels for ordering facet_grid()
sim_table$Trend.Strength = factor(sim_table$Trend.Strength, levels=c('ltrendstrong','ltrendmed','ltrendweak','notrend'))

sim_table$AR = factor(sim_table$AR, levels=c('NOAR','medAR','strongAR'))

#------------------Make figure---------------------#

ar_names <- c(`NOAR` = "No AR (phi = 0)",
                    `medAR` = "Med. AR (phi = 0.433)",
                    `strongAR` = "Strong AR (phi = 0.8)")

trend_names <- c(`ltrendstrong`="Strong trend",
                 `ltrendweak` = "Weak trend",
                 `notrend` = "No trend",
                 `ltrendmed` = "Med. trend")
#... + facet_grid(hospital ~ ., labeller = as_labeller(hospital_names))

ggplot(sim_table, aes(color = `Series length`,x = value, y = n)) +
  geom_bar(aes(fill = `Series length`),stat = "identity", position = "dodge", color = "steelblue",
           size = 0.1) +
  ylab("Fraction of Total Observations in Decile") +
  xlab("Decile") +
  scale_x_discrete(limits = c(1:10)) +
  facet_grid(Trend.Strength ~ AR,  labeller = labeller(Trend.Strength = as_labeller(trend_names),
                                                AR = as_labeller(ar_names))) +
  theme_bw()

```



##References