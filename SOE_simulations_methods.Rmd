---
title: "SOE Simulations Manuscript"
author: "Sean Hardison, Charles Perretti, Andy Beet, Geret DePiper"
date: "June 29, 2018"
output:
  pdf_document: default
  html_document: 
    df_print: paged
---

```{r setup, include=FALSE}
# rmarkdown::render("SOE_simulations_methods.Rmd", "all") # for both pdf and html

#data.dir <- '~/soe_simulations/data.dir/'
data.dir <- paste0(getwd(),"/data.dir/")
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

# list of all packages required
packages <- c("stringi","boot","tinytex","Kendall","zoo","zyp","trend","dplyr","AICcmodavg","nlme",
                      "gtools","tidyr","stringr","ggplot2","data.table","scales",
                      "RColorBrewer","colorspace","mccr","cowplot")
# may need to downgrade to rmarkdown 1.8.
# may need packages upquote, microtype, url, titling,mptopdf

installLoadPackages <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE,repos='http://cran.us.r-project.org')
    sapply(pkg, require, character.only = TRUE)
}

installLoadPackages(packages)

#
# library(boot);library(Kendall)
# library(zoo);library(zyp)
# library(trend);library(dplyr)
# library(AICcmodavg);library(nlme)
# library(gtools);library(tidyr)
# library(stringr);library(ggplot2)
# library(data.table)
# library(scales);library(RColorBrewer)
# library(colorspace);library(mccr)
# library(cowplot)

```

###Function
```{r functions, echo = F}
fit_lm <- function(dat, ar, m, ARsd, trend, spec = FALSE){
  success <- FALSE
  while(!success){
    dat <- dat %>% dplyr::filter(complete.cases(.))
    
    # Constant model (null model used to calculate 
    # overall p-value)
    constant_norm <-nlme::gls(series ~ 1, data = dat)
    
    #spec parameter specifies whether arima.sim incorporated AR(1) error process. When there is no 
    #AR error in the time series, we switch the GLS models to all rely on a normal error generating process.
    if (!spec){
      constant_ar1 <-
        try(nlme::gls(series ~ 1,
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(constant_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
      }
      
    } else {
      constant_ar1 <-
        try(nlme::gls(series ~ 1,
                      data = dat))
      if (class(constant_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
      }
    }
    # Linear model with normal error
    linear_norm <- nlme::gls(series ~ time, data = dat)
    
    # Linear model with AR1 error
    if (!spec){
      linear_ar1 <- 
        try(nlme::gls(series ~ time, 
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(linear_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
        }
      } else {
        linear_ar1 <- 
          try(nlme::gls(series ~ time, 
                        data = dat))
        if (class(linear_ar1) == "try-error"){
          return(best_lm <- data.frame(model = NA,
                                       aicc  = NA,
                                       coefs..Intercept = NA,
                                       coefs.time = NA,
                                       coefs.time2 = NA,
                                       pval = NA))
      }
    }
    linear_phi <- linear_ar1$modelStruct$corStruct
    linear_phi <-coef(linear_phi, unconstrained = FALSE)
    
    # Polynomial model with normal error
    dat$time2 <- dat$time^2
    poly_norm <- nlme::gls(series ~ time + time2, data = dat)
    
    # Polynomial model with AR1 error
    if (!spec){
      poly_ar1 <- 
        try(nlme::gls(series ~ time + time2, 
                      data = dat,
                      correlation = nlme::corAR1(form = ~time)))
      if (class(poly_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
        
      }
    }else {
      poly_ar1 <- 
        try(nlme::gls(series ~ time + time2, 
                      data = dat))
      if (class(poly_ar1) == "try-error"){
        return(best_lm <- data.frame(model = NA,
                                     aicc  = NA,
                                     coefs..Intercept = NA,
                                     coefs.time = NA,
                                     coefs.time2 = NA,
                                     pval = NA))
        
      }
    }
    poly_phi <- poly_ar1$modelStruct$corStruct
    poly_phi <- coef(poly_phi, unconstrained = FALSE)
    
    # Calculate AICs for all models
    df_aicc <-
      data.frame(model = c("poly_norm",
                           "poly_ar1",
                           "linear_norm",
                           "linear_ar1"),
                 aicc  = c(AICc(poly_norm),
                           AICc(poly_ar1),
                           AICc(linear_norm),
                           AICc(linear_ar1)),
                 coefs = rbind(coef(poly_norm),
                               coef(poly_ar1),
                               c(coef(linear_norm), NA),
                               c(coef(linear_ar1),  NA)),
                 phi = c(0, 
                         poly_phi,
                         0,
                         linear_phi),
                 # Calculate overall signifiance (need to use
                 # ML not REML for this)
                 pval = c(anova(update(constant_norm, method = "ML"),
                                update(poly_norm, method = "ML"))$`p-value`[2],
                          anova(update(constant_ar1, method = "ML"),
                                update(poly_ar1, method = "ML"))$`p-value`[2],
                          anova(update(constant_norm, method = "ML"),
                                update(linear_norm, method = "ML"))$`p-value`[2],
                          anova(update(constant_ar1, method = "ML"),
                                update(linear_ar1, method = "ML"))$`p-value`[2]))
    
    best_lm <-
      df_aicc %>%
      dplyr::filter(aicc == min(aicc))
    if (nrow(best_lm) >1){
      best_lm <- best_lm[1,]
    }
    phi <- best_lm$phi
    success <- (phi <= 0.8 & !invalid(phi))
    
    
    dat <- trend + dat
    dat <- arima.sim(list(ar = ar), n=m, rand.gen=rnorm, sd = ARsd)
    dat <- data.frame(series = dat,
                      time = 1:length(dat))
    
  }
  if (best_lm$model == "poly_norm") {
    model <- poly_norm
  } else if (best_lm$model == "poly_ar1") {
    model <- poly_ar1
  } else if (best_lm$model == "linear_norm") {
    model <- linear_norm
  } else if (best_lm$model == "linear_ar1") {
    model <- linear_ar1
  }
  return(list(best_lm = best_lm, 
              model = model))
}
```

##Methods
####Simulations
Simulated time series were generated through the addition of $AR(1)$ autoregressive processes to first-order linear models:

$$y = X\beta + \varepsilon_{t}, \:\: \mathrm{where} \:\: \varepsilon_{t} = \phi\varepsilon_{t-1} + \nu_{t}$$

where $X$ is the $n\:\times\:p$ model matrix, $\beta$ is a vector of model coefficients, and $\varepsilon_{t}$ is the $AR1$ error process; the strength of which is given by $\phi$. $\nu_{t}$ is assumed to be derived from Gaussian white noise. The levels of $\beta$ in our study were 0.004, .051, and .147, which we combined with four levels of $\phi$: 0, .43, .8, .95. These levels were chosen based on a preliminary analysis characterizing the distribution of trend and autocorrelation strengths across 2017 State of the Ecosystem time series data. 1000 simulations were implemented for all combinations of trend and autocorrelation strength. To test the null hypothesis of no trend in simulated time series, we used Generalized Least Squares, Mann Kendall test, and Mann Kendall test with trend-free pre-whitening. 

####Generalized least squares

The Generalized Least Squares (GLS) model fitting process used here was an iterative model-selection approach, where four GLS models were fit to each simulated time series. Specifically, two first-order and two second-order (quadratic) models were used. One first-order and one second-order GLS model were specified with first-order autocorrelated error structure identical to the error process used to generate simulations. 

Under $AR(1)$ error structure, the error-covariance matrix $\Sigma$ of the GLS estimator of $\beta$ is estimated by $\Sigma = \sigma^{2}P$, where $P$ is a diagonal matrix composed of error variances and autocorrelations from the data at different lag times ($\rho_{s}$). Error autocorrelations $\rho_{s}$ were estimated by restricted maximum-likelihood (REML) using the \textit{nlme} R package. The GLS estimator $b_{GLS}$ is given by

$$b_{GLS} = (X'\Sigma^{-1} X)^{-1}X'\Sigma^{-1}y,$$

where $y$ is the response vector, and $X$ is an $n\:\times\:p$ model matrix. The covariance matrix of $b_{GLS}$ is

$$b_{GLS} = (X'\Sigma^{-1}X)^{-1}.$$

The other first and second-order GLS models were specified with normal error structure, $N(\mu,\sigma^{2})$. The best fitting model for each simulation was selected based on small sample AIC (AICc), and we performed likelihood ratio tests between parameterized and null models specifying a maximum-likelihood approach to test the null of no trend given $\alpha = 0.05$.

####Mann Kendall test
Further tests for trend in simulated time series were performed using the Mann-Kendall test (MK) (Mann 1945; Kendall 1975) and the more robust Mann-Kendall test with trend-free pre-whitening (MK-TFPW) (Yue et al. 2002). The MK test is a non-parametric test for trend that assumes sample data are independent and identically distributed; an assumption frequently violated in time series data. Serial correlation within sample data will lead to inflated rejection rates of the null hypothesis of no trend if no correction steps are applied to the MK test (von Storch 1995), such as residual pre-whitening, although pre-whitening is known to reduce the magnitude of existing trend (Yue et al. 2002). The Mann-Kendall with trend-free pre-whitening is a step-wise procedure developed by Yue et al. 2002 to address issues introduced by pre-whitening, and is further detailed below. Under both MK and MK-TFPW frameworks, Kendall's tau statistic is given by:

$$S = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\textrm{sgn}(y_{j} - y_{i}),$$
where $y$ is the response vector, $n$ is the length of the series, and

$$\textrm{sgn}(x)=\begin{Bmatrix}
1 & \textrm{if}\:x>0\\ 
0 & \textrm{if}\:x=0\\ 
-1 & \textrm{if}\:x<0
\end{Bmatrix}.$$

When there are no ties in the data, the variance of $S$ is given by 

$$V(S) = \frac{n(n-1)(2n+5)}{18},$$
and the distribution of $S$ when $n \geq 8$ is approximately normal and symmetric about a mean of 0. We then perform a two-sided test for trend using the standardized $Z$ statistic:

$$\textrm{Z}=\begin{Bmatrix}
\frac{S-1}{\sqrt{V(S)}} & S>0\\ 
0 & S=0\\ 
\frac{S+1}{\sqrt{V(S)}} & S<0\\ 
\end{Bmatrix},$$

which is drawn from a normal distribution with mean of zero and variance of one. Following Yue et al. 2002, a $P$ value for the test is determined using the standard normal cumulative distribution function

$$P = \int_{\infty}^{Z}e^{-t^{2}/2}\mathrm{d}t.$$
For significance level $\alpha$, if $Z_{a/2} > |P|$, then we reject $H_{0}$ of no trend.  


####Mann-Kendall trend-free pre-whitening (MK-TFPW)
The Mann Kendall trend-free pre-whitening procedure as developed by Yue et al. 2002 is composed of four steps:
```{r render list, results='asis', eval = T, echo = F}

if(knitr::is_latex_output()){
  cat("\\begin{enumerate}\\item \\textit{Removal of trend} - The slope of trend $b$ is estimated using the Theil-Sen estimator (Theil 1950a-c; Sen 1968) and removed from sample data if different from zero, where $b$ is given by
$$b = \\mathrm{Median}\\left (\\frac{X_{j} - X_{i}}{j - l}\\right)\\forall l < j .$$
Trend $b$ is removed from the series by
$$X_{t}^{'} = X_{t} - bt,$$
where $X_{t}$ is the original series at time step $t$.
\\item \\textit{Trend-free pre-whitening} - A pre-whitening step is applied to the detrended series to remove the $AR(1)$ component. First, the lag-$1$ autocorrelation coefficient $\\rho_{1}$ is computed using 
$$\\rho_{k} = \\frac{  \\frac{1}{n-k} \\sum_{t=1}^{n-k}[X_{t} - E(X_{t})][X_{t+k} - E(X_{t})]}  {\\frac{1}{n}\\sum_{t=1}^{n}[X_{t} - E(X_{t})]^2},$$
where $E(X_{t})$ is the mean of the series and $\\rho_{k}$ is the lag-$k$ autocorrelation coefficient. Serial correlation is then removed from the detrended series $X_{t}^{'}$ by 
$$Y_{t}^{'} = X_{t}^{'} - \\rho_{1}X_{t}^{'}.$$
\\item \\textit{Blending trend and residual series} - Trend $b$ is added to the independent residual series $Y_{t}^{'}$ by 
$$Y_{t} = Y_{t}^{'} + bt.$$
\\item \\textit{MK test} - Trend is assessed through the application of the Mann Kendall test as discussed above.

\\end{enumerate}")
} else if (knitr::is_html_output()){
    cat("1. *Removal of trend* - The slope of trend $b$ is estimated using the Theil-Sen estimator (Theil 1950a-c; Sen 1968) and removed from sample data if different from zero, where $b$ is given by
$$b = \\mathrm{Median}\\left (\\frac{X_{j} - X_{i}}{j - l}\\right)\\forall l < j .$$
Trend $b$ is removed from the series by
$$X_{t}^{'} = X_{t} - bt,$$
where $X_{t}$ is the original series at time step $t$.
2. *Trend-free pre-whitening* - A pre-whitening step is applied to the detrended series to remove the $AR(1)$ component. First, the lag-$1$ autocorrelation coefficient $\\rho_{1}$ is computed using 
$$\\rho_{k} = \\frac{  \\frac{1}{n-k} \\sum_{t=1}^{n-k}[X_{t} - E(X_{t})][X_{t+k} - E(X_{t})]}  {\\frac{1}{n}\\sum_{t=1}^{n}[X_{t} - E(X_{t})]^2},$$
where $E(X_{t})$ is the mean of the series and $\\rho_{k}$ is the lag-$k$ autocorrelation coefficient. Serial correlation is then removed from the detrended series $X_{t}^{'}$ by 
$$Y_{t}^{'} = X_{t}^{'} - \\rho_{1}X_{t}^{'}.$$
3. *Blending trend and residual series* - Trend $b$ is added to the independent residual series $Y_{t}^{'}$ by 
$$Y_{t} = Y_{t}^{'} + bt.$$
4. *MK test* - Trend is assessed through the application of the Mann Kendall test as discussed above.")
}

```




##Results
```{r constants for simulations, echo = F}
set.seed(123)
run <- F #Run simulations or pull from previous results

nsims = 500 #number of simulations
ARsd <- .54^.5 #standard deviation of innovations

NOAR <- list()
weakAR <- 0.1
medAR <- 0.433
strongAR <- 0.8

date <- "2018-06-29"
x <- 30
```


####Assessing power of trend detection tests under varying levels of autocorrelation, trend strength, and time series lengths. 
```{r assessing model power, echo = F}
if (run){
  # Trend parameters are from fitting a linear model to the
# z-scored real data: 5th percentile (weak), mean (medium), 
# 95th (strong). The mean intercept of the z-scored real
# data was -0.262.
LTRENDweak   <- -0.262 + (0.004 * c(1:x)) #Linear trend
LTRENDmedium <- -0.262 + (0.051 * c(1:x)) 
LTRENDstrong <- -0.262 + (0.147 * c(1:x))
# AR values are from the real data by fitting an ar1
# model to the resids of the linear fit. The mean of
# those values was 0.433. We also include 0.8 to 
# investigate the effect of strong autocorrelation.
ARmedium <- list(ar = medAR)
ARstrong <- list(ar = strongAR)
# Mean AR sd from the mean resids of the real data
ARsd <- 0.54^0.5
NOAR <- list()
m <- x
#Adding placeholders for simulated data
LINEARweak_ARmedium <- NULL 
LINEARweak_ARstrong <- NULL 
LINEARmedium_ARmedium <- NULL 
LINEARmedium_ARstrong <- NULL
LINEARstrong_ARmedium <- NULL 
LINEARstrong_ARstrong <- NULL 

NOTREND_ARmedium <- NULL
NOTREND_ARstrong <- NULL
NOTREND_NOAR <- NULL

LINEARweak_NOAR <- NULL
LINEARmedium_NOAR <- NULL
LINEARstrong_NOAR <- NULL

NOTREND_ARmedium_RESULTS <- NULL
NOTREND_NOAR_RESULTS <- NULL
NOTREND_ARstrong_RESULTS <- NULL
LINEARweak_ARmedium_RESULTS <- NULL
LINEARweak_ARstrong_RESULTS <- NULL
LINEARmedium_ARmedium_RESULTS <- NULL
LINEARmedium_ARstrong_RESULTS <- NULL
LINEARstrong_ARmedium_RESULTS <- NULL
LINEARstrong_ARstrong_RESULTS <- NULL
LINEARweak_NOAR_RESULTS <- NULL
LINEARmedium_NOAR_RESULTS <- NULL
LINEARstrong_NOAR_RESULTS <- NULL


#initializing simulations
for (i in 1:nsims) {
  #Generating ar(1) simulations
  for (k in c('ARmedium','ARstrong','NOAR')){
    # Simulate arima process with sd set to the mean sd
    # of the residuals
    TEMP1 <- arima.sim(get(k), n=x, rand.gen=rnorm, sd = ARsd)
    LTEMP1 <- TEMP1 + LTRENDweak
    LTEMP2 <- TEMP1 + LTRENDmedium
    LTEMP3 <- TEMP1 + LTRENDstrong
    assign(paste0('LINEARweak_',k,sep=""),rbind(get(paste0('LINEARweak_',k,sep="")),LTEMP1))
    assign(paste0('LINEARmedium_',k,sep=""),rbind(get(paste0('LINEARmedium_',k,sep="")),LTEMP2))
    assign(paste0('LINEARstrong_',k,sep=""),rbind(get(paste0('LINEARstrong_',k,sep="")),LTEMP3))
    assign(paste0('NOTREND_',k, sep=""),rbind(get(paste0('NOTREND_',k, sep="")), TEMP1))
    
    for (y in c('TEMP1','LTEMP1','LTEMP2','LTEMP3')){
      # Print counter
      
      #Testing 30 year series with prewhitening Mann-Kendall technique
      TEMP_TEST1 <- zyp.trend.vector(get(y),method='yuepilon')
      TEMP_P1 <- unlist(TEMP_TEST1[6])
      TEMP_TREND1 <- unlist(TEMP_TEST1[3])
      T_1 <- cbind(TEMP_P1,TEMP_TREND1)
      #30 year standard Mann-Kendall
      TEMP_TEST12 <- zyp.trend.vector(get(y),method='zhang')
      T_2 <- unlist(TEMP_TEST12[6])
      # 30-year linear model
      
      if (y == "TEMP1"){
        trend <- rep(0,x)
      } else if (y == "LTEMP1"){
        trend <- LTRENDweak
      } else if (y == "LTEMP2"){
        trend <- LTRENDmedium
      } else if (y == "LTEMP3"){
        trend <- LTRENDstrong
      }
      
      if (k == "NOAR"){
        TEMP_lm <- fit_lm(dat = data.frame(series = get(y) %>% as.numeric,
                                         time = 1:length(get(y))),
                          spec = TRUE, trend = trend, ar = unlist(get(k)),
                          m = x, ARsd = ARsd)
      } else {
       TEMP_lm <- fit_lm(dat = data.frame(series = get(y) %>% as.numeric,
                                         time = 1:length(get(y))), trend = trend,
                         ar = unlist(get(k)),m = x, ARsd = ARsd)
      }
      
      
      
      
      T_lm <- TEMP_lm$best_lm$pval
      
      
      for (j in c(10,20)) {
        #Testind 20 & 10 year series with prewhitening Mann-Kendall technique
        TEMP_TEST2 <- zyp.trend.vector(get(y)[j:x],method='yuepilon')
        TEMP_P2 <- unlist(TEMP_TEST2[6])
        TEMP_TREND2 <- unlist(TEMP_TEST2[3])
        T_1 <- cbind(T_1,TEMP_P2,TEMP_TREND2)
        #Now standard Mann_Kendall
        TEMP_TEST22 <- zyp.trend.vector(get(y)[j:x],method='zhang')
        TEMP_P22 <- unlist(TEMP_TEST22[6])
        T_2 <- cbind(T_2,TEMP_P22)
        # 20-year and 10-year linear model
      
      if (k == "NOAR"){
        TEMP_lm <- fit_lm(dat = data.frame(series = get(y)[j:x] %>% as.numeric,
                                           time = 1:length(get(y)[j:x])),
                          spec = TRUE, trend = trend, ar = unlist(get(k)),
                          m = length(get(y)[j:x]), ARsd = ARsd)
      } else {
       TEMP_lm <- fit_lm(dat = data.frame(series = get(y)[j:x] %>% as.numeric,
                                           time = 1:length(get(y)[j:x])), trend = trend,
                         ar = unlist(get(k)),m = length(get(y)[j:x]), ARsd = ARsd)
      }
        T_lm <- cbind(T_lm, TEMP_lm$best_lm$pval)
        # 20-year and 10-year gam
        #TEMP_gam <- fit_gam(dat = data.frame(series = get(y)[j:x] %>% as.numeric,
        #                                     time = 1:length(get(y)[j:x])))
        #T_gam <- cbind(T_gam, TEMP_gam$pval)
      }
      colnames(T_1) <- c('p_30_pw','Slope30_pw','p_20_pw',
                         'Slope20_pw','p_10_pw','Slope10_pw')
      colnames(T_2) <- c('p_30_mk','p_20_mk','p_10_mk')
      colnames(T_lm) <- c('p_30_gls','p_20_gls','p_10_gls')
      
      if (y=='TEMP1' & k!='NOAR') {assign(paste0('NOTREND_',k,'_RESULTS',sep=""),
                                          rbind(get(paste0('NOTREND_',k,'_RESULTS',sep="")),
                                                cbind(T_1,T_2, T_lm)))
      } else if (y=='TEMP1' & k=='NOAR') {assign(paste0('NOTREND_',k,'_RESULTS',sep=""),
                                                 rbind(get(paste0('NOTREND_',k,'_RESULTS',sep="")),
                                                       cbind(T_1,T_2, T_lm)))
      } else if (y=='LTEMP1') {assign(paste0('LINEARweak_',k,'_RESULTS',sep=""),
                                      rbind(get(paste0('LINEARweak_',k,'_RESULTS',sep="")),
                                            cbind(T_1,T_2, T_lm)))
      } else if (y=='LTEMP2') {assign(paste0('LINEARmedium_',k,'_RESULTS',sep=""),
                                      rbind(get(paste0('LINEARmedium_',k,'_RESULTS',sep="")),
                                            cbind(T_1,T_2, T_lm)))
      } else if (y=='LTEMP3') {assign(paste0('LINEARstrong_',k,'_RESULTS',sep=""),
                                      rbind(get(paste0('LINEARstrong_',k,'_RESULTS',sep="")),
                                            cbind(T_1,T_2, T_lm)))}
    }
    #rm(TEMP1,LTEMP1,QTEMP1)
  }
}

#convert Monte Carlo simulation output (p values and slopes)
# from wide to long format for plotting
library(data.table)
results <- c("NOTREND_ARmedium_RESULTS", "NOTREND_NOAR_RESULTS", "NOTREND_ARstrong_RESULTS",
             "LINEARweak_ARmedium_RESULTS", "LINEARweak_ARstrong_RESULTS",
             "LINEARmedium_ARmedium_RESULTS","LINEARmedium_ARstrong_RESULTS",
             "LINEARstrong_ARmedium_RESULTS", "LINEARstrong_ARstrong_RESULTS",
             "LINEARweak_NOAR_RESULTS","LINEARmedium_NOAR_RESULTS","LINEARstrong_NOAR_RESULTS")


#wide to long function (Sean Lucey)
w2l <- function(x, by, by.name = 'Time', value.name = 'Value'){
  x.new <- copy(x)
  var.names <- names(x)[which(names(x) != by)]
  out <- c()
  setnames(x.new, by, 'by')
  for(i in 1:length(var.names)){
    setnames(x.new, var.names[i], 'V1')
    single.var <- x.new[, list(by, V1)]
    single.var[, Var := var.names[i]]
    out <- rbindlist(list(out, single.var))
    setnames(x.new, 'V1', var.names[i])
  }
  setnames(x.new, 'by', by)
  setnames(out, c('by', 'V1'), c(by.name, value.name))
}

#filter for p values
p_result_mat <- list()
for (i in 1:length(results)){
  z <- get(results[i])
  z <- cbind(z[,grepl("p_",colnames(get(results[i]))) == TRUE],seq(1,nrow(z),1))
  z <- data.table(z)
  z <- w2l(z, by = "V10", by.name = "replicate")
  z <- cbind(z,rep(results[i],nrow(z)))
  p_result_mat[[i]] <- z
}

# Final p dataframe
p_results <- 
  do.call(rbind, p_result_mat) %>%
  as.data.frame() %>%
  tidyr::separate(Var, 
                  c("var", "timeseries length", "method"),
                  "_") %>%
  tidyr::separate(V2, 
                  c("Trend strength", "AR strength", "Results"),
                  "_") %>%
  dplyr::mutate(`Trend strength` = substring(`Trend strength`, first = 7),
                `Trend strength` = ifelse(`Trend strength` == "D",
                                          "no",
                                          `Trend strength`),
                `Trend strength` = paste0(`Trend strength`, " trend"),
                `AR strength` = substring(`AR strength`, first = 3),
                `AR strength` = ifelse(`AR strength` == "AR",
                                       "no",
                                       `AR strength`),
                `AR strength` = paste0(`AR strength`, " AR")) %>%
  dplyr::select(-Results) %>%
  tidyr::spread(method, Value) %>%
  # remove runs that cause NAs in GLS (two runs out of 1000)
  dplyr::filter(!is.na(gls)) %>%
  tidyr::gather(method, Value, 
                -replicate, -var, 
                -`timeseries length`, -`Trend strength`,
                -`AR strength`) %>%
  dplyr::mutate(`Trend strength` = factor(`Trend strength`, 
                                          levels = c("strong trend",
                                                     "medium trend",
                                                     "weak trend",
                                                     "no trend")),
                `AR strength` = factor(`AR strength`, 
                                       levels = c("no AR",
                                                  "medium AR",
                                                  "strong AR")))
print( paste0(data.dir,"/power_results","_",Sys.Date(),".csv"))
  write.csv(p_results, file = paste0(data.dir,"power_results","_",Sys.Date(),".csv"))
} else if (!run){
  p_results <- read.csv(paste0(data.dir,"power_results","_",date,".csv"))
}





```

```{r power analysis figures, echo = F, fig.align='center'}
p_agg <- p_results %>% group_by(timeseries.length, Trend.strength, AR.strength,
                        method) %>% dplyr::summarise(median = median(Value),
                                                     mean = mean(Value),
                                                     sd = sd(Value),
                                                     n = n(),
                                                     prop = length(Value[Value < 0.05])/n())
p_agg$Trend.strength = factor(p_agg$Trend.strength, levels=c('strong trend','medium trend','weak trend','no trend'))

p_agg$AR.strength = factor(p_agg$AR.strength, levels=c('no AR','medium AR','strong AR'))

ggplot(p_agg, aes(color = method, y = prop, 
                      x = timeseries.length)) +
  geom_bar(stat = "identity", position = "dodge",
           fill = "grey95", size = 0.75) +
  facet_grid(Trend.strength ~ AR.strength) +
  ylab(expression(paste("Power (", N[Rej],"/",N[Tot],")",sep=""))) +
  xlab("Time series length") +
  theme_bw()

```


####GLS decile coverage simulations

```{r decile coverage simulations, echo = F}

#placeholders for results
gls.ts.NOAR.notrend <- NULL
gls.ts.NOAR.ltrendweak <- NULL
gls.ts.NOAR.ltrendmed <- NULL
gls.ts.NOAR.ltrendstrong <- NULL

gls.ts.medAR.notrend <- NULL
gls.ts.medAR.ltrendweak <- NULL
gls.ts.medAR.ltrendmed <- NULL
gls.ts.medAR.ltrendstrong <- NULL

gls.ts.strongAR.notrend <- NULL
gls.ts.strongAR.ltrendweak <- NULL
gls.ts.strongAR.ltrendmed <- NULL
gls.ts.strongAR.ltrendstrong <- NULL

sim_results_10 <- NULL
sim_results_20 <- NULL
sim_results_30 <- NULL


if (run){
  ptm <- proc.time()
  #Specify time series length
  for (m in c(10,20,30)){
  
  notrend <- rep(0,m)
  ltrendweak <- -0.262 + (0.004 * c(1:m)) 
  ltrendmed <- -0.262 + (0.051 * c(1:m)) 
  ltrendstrong <- -0.262 + (0.147 * c(1:m)) 
  print(paste("m=",m))

    #Trend strength
    for (k in c("notrend","ltrendweak","ltrendmed","ltrendstrong")){
    
    #AR strength
    for (j in c("strongAR","medAR","NOAR")){
    
      true_trend <- get(k)
      
      for (i in 1:nsims){
        
        #generate simulations
        dat <- arima.sim(list(ar = get(j)), n=m, rand.gen=rnorm, sd = ARsd)
        
        #add autocorrelated error structure to trend
        dat <- get(k) + dat
        dat <- data.frame(series = dat,
                          time = 1:length(dat))
        
        #---------------------------------GLS---------------------------------#
        gls_sim <- tryCatch({
          newtime <- seq(1, m, 1)
          newdata <- data.frame(time = newtime,
                                time2 = newtime^2)
          
          #Correctly specifies model when no AR error in simulated time series
          if (j == "NOAR"){
            gls_sim <- fit_lm(dat = dat, ar = get(j),
                              ARsd = ARsd, m = m, trend = get(k), spec = TRUE)
          } else{
            gls_sim <- fit_lm(dat = dat, ar = get(j), ARsd = ARsd, m = m, trend = get(k))
            
          }
          
        }, 
        error = function(e) {
          gls_sim <- "error"
        })
        
        
        #---------------------------------Decile coverage---------------------------------#
        if (is.na(gls_sim[1]) | gls_sim[1] == "error"){
          
          gls_pred <- data.frame(fit = rep(NA, m))
          decile_of_true <- rep(NA, m)
        } else {
          gls_pred <- AICcmodavg::predictSE(gls_sim$model,
                                            newdata = newdata,
                                            se.fit = TRUE)
          decile_of_true <- ceiling(10 * pnorm(q    = true_trend, 
                                           mean = gls_pred$fit, 
                                           sd   = gls_pred$se.fit))
        }
        
        

        
        for (g in 1:m){
          if (is.na(gls_pred$fit[1])){
            print("NA")
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),NA))
          } else {
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),decile_of_true[g]))
          }
          
        }
          
        
      } 
      
    }
  }
  sim_results = data.frame(gls.NOAR.ltrendweak = gls.ts.NOAR.ltrendweak,
                           gls.NOAR.ltrendmed = gls.ts.NOAR.ltrendmed,
                           gls.NOAR.ltrendstrong = gls.ts.NOAR.ltrendstrong,
                           gls.NOAR.notrend = gls.ts.NOAR.notrend,

                           gls.medAR.ltrendweak = gls.ts.medAR.ltrendweak,
                           gls.medAR.ltrendmed = gls.ts.medAR.ltrendmed,
                           gls.medAR.ltrendstrong = gls.ts.medAR.ltrendstrong,
                           gls.medAR.notrend = gls.ts.medAR.notrend,

                           gls.strongAR.ltrendweak = gls.ts.strongAR.ltrendweak,
                           gls.strongAR.ltrendmed = gls.ts.strongAR.ltrendmed,
                           gls.strongAR.ltrendstrong = gls.ts.strongAR.ltrendstrong,
                           gls.strongAR.notrend = gls.ts.strongAR.notrend)

    assign(paste0('sim_results_',m), rbind(get(paste0('sim_results_',m)),sim_results))
    #write.csv(sim_results, file = paste0("decile_coverage_results_",m,Sys.Date(),".csv"))
    
    gls.ts.NOAR.notrend <- NULL
    gls.ts.NOAR.ltrendweak <- NULL
    gls.ts.NOAR.ltrendmed <- NULL
    gls.ts.NOAR.ltrendstrong <- NULL
    
    gls.ts.medAR.notrend <- NULL
    gls.ts.medAR.ltrendweak <- NULL
    gls.ts.medAR.ltrendmed <- NULL
    gls.ts.medAR.ltrendstrong <- NULL
    
    gls.ts.strongAR.notrend <- NULL
    gls.ts.strongAR.ltrendweak <- NULL
    gls.ts.strongAR.ltrendmed <- NULL
    gls.ts.strongAR.ltrendstrong <- NULL
    print(proc.time() - ptm)
}

coverage_10 <- write.csv(sim_results_10, file = paste0(data.dir,"coverage_10","_",date,".csv"))
coverage_20 <- write.csv(sim_results_20, file = paste0(data.dir,"coverage_20","_",date,".csv"))
coverage_30 <- write.csv(sim_results_30, file = paste0(data.dir,"coverage_30","_",date,".csv"))
  
} else if (!run){

  coverage_10 <- read.csv(file = paste0(data.dir,"coverage_10","_",date,".csv"))
  coverage_20 <- read.csv(file = paste0(data.dir,"coverage_20","_",date,".csv"))
  coverage_30 <- read.csv(file = paste0(data.dir,"coverage_30","_",date,".csv"))
}


```

```{r decile coverage plots, echo = F, fig.align='center'}

#---------------------------Process data--------------------------#
sim_30 <- tidyr::gather(coverage_30, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_30$`Series length` <- "30"
sim_20 <- tidyr::gather(coverage_20, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_20$`Series length` <- "20"
sim_10 <- tidyr::gather(coverage_10, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_10$`Series length` <- "10"
sims <- rbind(sim_10, sim_20, sim_30)

sim_table <- sims %>% group_by(`Series length`, var,value)  %>%
  filter(value != 0) %>%
  tally() %>%
  group_by(`Series length`) %>% mutate(n = n/(as.numeric(`Series length`)*nsims))#freq. table


#------------------Split out columns for grouping----------------#
col <- do.call(rbind.data.frame, str_split(sim_table$var, '[.]'))
names(col) <- c("Test","AR","Trend Strength")
sim_table$Test <- col$Test
sim_table$AR <- col$AR
sim_table$Trend.Strength <- col$`Trend Strength`

#Set factor levels for ordering facet_grid()
sim_table$Trend.Strength = factor(sim_table$Trend.Strength, levels=c('ltrendstrong','ltrendmed','ltrendweak','notrend'))

sim_table$AR = factor(sim_table$AR, levels=c('NOAR','medAR','strongAR'))

#------------------Make figure---------------------#

ar_names <- c(`NOAR` = "No AR (phi = 0)",
                    `medAR` = "Med. AR (phi = 0.433)",
                    `strongAR` = "Strong AR (phi = 0.8)")

trend_names <- c(`ltrendstrong`="Strong trend",
                 `ltrendweak` = "Weak trend",
                 `notrend` = "No trend",
                 `ltrendmed` = "Med. trend")
#... + facet_grid(hospital ~ ., labeller = as_labeller(hospital_names))

ggplot(sim_table, aes(color = `Series length`,x = value, y = n)) +
  geom_bar(aes(fill = `Series length`),stat = "identity", position = "dodge", color = "steelblue",
           size = 0.1) +
  ylab("Fraction of Total Observations in Decile") +
  xlab("Decile") +
  scale_x_discrete(limits = c(1:10)) +
  facet_grid(Trend.Strength ~ AR,  labeller = labeller(Trend.Strength = as_labeller(trend_names),
                                                AR = as_labeller(ar_names))) +
  theme_bw()

```

####Confusion matrices and Matthew's correlation coefficient
```{r confusion matrices, echo = F, fig.align='center'}

label <- function(variable,value){
  return(facet_names[value])
}


summary_func <- function(df, full_proc = T, type){
  agg <- df %>% group_by(timeseries.length, Trend.strength, AR.strength,
                              method) %>%  
    dplyr::summarise(median = median(Value),
       mean = mean(Value),
       sd = sd(Value),
       n = n(),
       prop = length(Value[Value < 0.05])/n())
  
  if (full_proc & type == 2){
    agg <- agg %>% group_by(timeseries.length, method) %>%
      filter(Trend.strength != 'no trend') %>%
      dplyr::summarise(mean = mean(prop))
    return(agg)
  } else if (full_proc & type == 1){
    agg <- agg %>% group_by(timeseries.length, method) %>%
      filter(Trend.strength == 'no trend') %>%
    dplyr::summarise(mean = mean(prop))
    return(agg)
  }
    return(agg)
  }
 

false_pos_mat <- summary_func(p_results, type = 1)
false_rej_mat <- summary_func(p_results, type = 2)

#--------Confusion matrices - One per test----------#

#We want the percentage of true positives, false positives, true negatives, and false negatives

conf_mat <- function(df, test, ar = NULL){
  
  if (!is.null(ar)){
    ar <- ar
  } else{
    ar <- "AR"
  }
  #True positives
  true_pos_frac <- nrow(df[df$Trend.strength != "no trend" &
                          df$Value <= 0.05 &
                          df$method == test &
                            grepl(ar,df$AR.strength),])
  true_pos_tot <- nrow(df[df$Trend.strength != "no trend" &
                            df$method == test&
                            grepl(ar,df$AR.strength),])
  
  true_pos_freq <- true_pos_frac/true_pos_tot 
  
  #False positives
 
  false_pos_frac <- nrow(df[df$Trend.strength == "no trend" &
                        df$Value <= 0.05 &
                        df$method == test&
                          grepl(ar,df$AR.strength),])
  false_pos_tot <- nrow(df[df$Trend.strength == "no trend" &
                             df$method == test&
                             grepl(ar,df$AR.strength),])
  
  false_pos_freq <- false_pos_frac/false_pos_tot
  
  #False negatives

  false_neg_frac <- nrow(df[df$Trend.strength != "no trend" &
                         df$Value >= 0.05 &
                         df$method == test&
                           grepl(ar,df$AR.strength),])
  false_neg_tot <- nrow(df[df$Trend.strength != "no trend "&
                             df$method == test&
                             grepl(ar,df$AR.strength),])
  
  false_neg_freq <- false_neg_frac/false_neg_tot
  
  #true
  
  true_neg_frac <- nrow(df[df$Trend.strength == "no trend" &
                         df$Value >= 0.05 &
                         df$method == test&
                           grepl(ar,df$AR.strength),])
  true_neg_tot <- nrow(df[df$Trend.strength == "no trend" &
                            df$method == test&
                            grepl(ar,df$AR.strength),])
  
  true_neg_freq <- true_neg_frac/true_neg_tot

  
  conf_mat <- data.frame(x = c("actual no","actual yes","actual no","actual yes"),
                         y = c('predicted no','predicted yes','predicted yes','predicted no'),
                         val = c(round(true_neg_freq,3), round(true_pos_freq,3),
                                 round(false_pos_freq,3),round(false_neg_freq,3)))
  
  return(conf_mat)
}

mk <- cbind(conf_mat(p_results,test = "mk"), test = rep('mk',4))
pw <- cbind(conf_mat(p_results, test = "pw"), test = rep('pw',4))
gls <- cbind(conf_mat(p_results, test = "gls"), test = rep('gls',4))
fin <- rbind(mk, pw, gls)
fin$group <- factor(paste(fin$x,fin$y))


#Make matrices for white = good and orange = bad
fin_dif <- fin %>% group_by(group) %>%
  mutate(val, best_dif = ifelse(group == "actual no predicted no"|
                                  group == "actual yes predicted yes", (abs(max(val) - val)), #best_dif is for assigning colors
                                (abs(min(val) - val)))) 

#For custom facet titles
facet_names <- list(
  'mk'="Mann-Kendall",
  'pw'="MK-TFPW",
  'gls'="GLS"
)

pal <- colorRampPalette(c("white","darkorange")) #color palette

#plot
ggplot(data = fin_dif, aes(x,y, fill = best_dif)) +
  facet_grid(. ~ test, labeller = label)+
  geom_tile(aes(size = 1),color = "grey", size = 1)  +
  scale_fill_gradientn(colors = pal(10))+
  geom_text(aes(x = x, y = y, label = round(val,3), size = 1),size = 4) +
  theme(legend.position = "none",
        axis.line = element_blank(),
        axis.text=element_text(size=12),
        axis.title=element_blank(),
        axis.text.y = element_text(margin = margin(t = 0, r = -6,
                                                   b = 0, l = 0),
                                   size = 10),
        axis.text.x = element_text(margin = margin(t = -3, r = 0,
                                                   b = 150, l = 0),
                                   size = 10),
        axis.ticks.y=element_blank(),
        axis.ticks.x=element_blank(),
        plot.title = element_text(hjust = -0.1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())

# #This piece uses color to show divergence of cells from the mean of that cell class across models
# fin_dif <- fin %>% group_by(group) %>%
#   mutate(mean_dif = mean(val) - val)
# 
# fin_dif$group <- factor(fin_dif$group)
# 
# ggplot(data = fin_dif, aes(x,y, fill = mean_dif)) +
#   facet_grid(. ~ test, labeller = label)+
#   geom_tile(aes(size = 0.25))  +
#   scale_fill_gradient2(low = "darkorange", mid = "white", high = "purple")+
#   geom_text(aes(x = x, y = y, label = round(val,3), size = 1.4)) +
#   theme(legend.position = "none",
#         axis.line = element_blank(),
#         axis.text=element_text(size=12),
#         axis.title=element_blank(),
#         axis.text.y = element_text(margin = margin(t = 0, r = -10,
#                                                    b = 0, l = 0),
#                                    size = 15),
#         axis.text.x = element_text(margin = margin(t = -10, r = 0,
#                                                    b = 0, l = 0),
#                                    size = 15),
#         axis.ticks.y=element_blank(),
#         axis.ticks.x=element_blank(),
#         plot.title = element_text(hjust = -0.1))
# 
# 
# #Do the same for varying autocorrelation strengths - first strong AR class
# 
# mk <- cbind(conf_mat(p_results,test = "mk", ar = "strong"), test = rep('mk',4))
# pw <- cbind(conf_mat(p_results, test = "pw", ar = "strong"), test = rep('pw',4))
# gls <- cbind(conf_mat(p_results, test = "gls", ar = "strong"), test = rep('gls',4))
# 
# 
# ar_str <- rbind(mk, pw, gls)
# ar_str$group <- paste(ar_str$x,ar_str$y)
# 
# ar_str_dif <- ar_str %>% group_by(group) %>%
#   mutate(mean_dif = mean(val) - val)
# 
# 
# ar_str_dif$group <- factor(ar_str_dif$group)
# 
# ggplot(data = ar_str_dif, aes(x,y, fill = mean_dif)) +
#   facet_grid(. ~ test, labeller = label)+
#   geom_tile(aes(size = 0.25))  +
#   scale_fill_gradient2(low = "darkorange", mid = "white", high = "purple")+
#   geom_text(aes(x = x, y = y, label = round(val,3), size = 1.4)) +
#   theme(legend.position = "none",
#         axis.line = element_blank(),
#         axis.text=element_text(size=12),
#         axis.title=element_blank(),
#         axis.text.y = element_text(margin = margin(t = 0, r = -10,
#                                                    b = 0, l = 0),
#                                    size = 15),
#         axis.text.x = element_text(margin = margin(t = -10, r = 0,
#                                                    b = 0, l = 0),
#                                    size = 15),
#         axis.ticks.y=element_blank(),
#         axis.ticks.x=element_blank(),
#         plot.title = element_text(hjust = -0.1))

```

```{r MCCR, echo = F, fig.align='center',fig.width = 8, fig.height= 3.25}
#Matthew correlation coefficient
p_resultsm <- p_results %>% mutate(Trend.strength,
                                   actual = plyr::mapvalues(Trend.strength, from = c("strong trend","medium trend",
                                                                                       "weak trend", "no trend"),
                                             to = c(1,1,1,0))) %>%
  mutate(predict = ifelse(Value < 0.05,1,0 ))

mk <- p_resultsm[p_resultsm$method == 'mk',]
pw <- p_resultsm[p_resultsm$method == 'pw',]
gls <- p_resultsm[p_resultsm$method == 'gls',]

#plotting MCCR across AR strengths
df <- p_resultsm
mccr_ar <- function(df,ar,time = NULL){
  if(!is.null(time)){
    z <- df[df$timeseries.length == time,]
    z_mccr <- mccr(z$actual, z$predict)
    return(as.numeric(z_mccr)) 
  } else {
    z <- df[df$AR.strength == ar,]
    z_mccr <- mccr(z$actual, z$predict)
    return(as.numeric(z_mccr))  
  }
 
}

mcc_mk <- data.frame(mcc = c(mccr_ar(mk, "no AR"),mccr_ar(mk, "medium AR"),mccr_ar(mk, "strong AR")),
                var = c("no AR","med. AR AR","strong AR"),
                Test = "Mann-Kendall",
                id = "AR Strength")
mcc_pw <- data.frame(mcc = c(mccr_ar(pw, "no AR"),mccr_ar(pw, "medium AR"),mccr_ar(pw, "strong AR")),
                var = c("no AR","med. AR","strong AR"),
                Test = "MK-TFPW",
                id = "AR Strength")
mcc_gls <- data.frame(mcc = c(mccr_ar(gls, "no AR"),mccr_ar(gls, "medium AR"),mccr_ar(gls, "strong AR")),
                 var = c("no AR","med. AR","strong AR"),
                 Test = "GLS",
                 id = "AR Strength")

mcc.ar <- rbind(mcc_mk, mcc_pw, mcc_gls)

ar <- ggplot(data = mcc.ar, aes(x = var, y = mcc, group = Test))+
  geom_line(aes(color = Test), size = 1.1) +
  geom_point(aes(color = Test), size = 1.5) +
  scale_x_discrete(limits=c("no AR","med. AR","strong AR"))+
  labs(x = "Autocorrelation strength",
       y = "MCC") +
  theme(axis.text = element_text(colour="grey20",size=11,hjust=.5,vjust=.5,face="plain"),
        axis.title.x = element_text(colour="grey20",size=17,angle=0,vjust=-1,face="plain"),
        axis.title.y = element_text(colour="grey20",size=17,face="plain"),
        legend.position = "none")

#MCCR across time series lengths

mcc_mk_time <- data.frame(mcc = c(mccr_ar(mk, time = 10),mccr_ar(mk, time = 20),mccr_ar(mk, time = 30)),
                     var = c(10,20,30),
                     Test = "Mann-Kendall",
                     id = "Series Length")
mcc_pw_time <- data.frame(mcc = c(mccr_ar(pw, time = 10),mccr_ar(pw, time = 20),mccr_ar(pw, time = 30)),
                          var = c(10,20,30),
                     Test = "MK-TFPW",
                     id = "Series Length")
mcc_gls_time <- data.frame(mcc = c(mccr_ar(gls, time = 10),mccr_ar(gls, time = 20),mccr_ar(gls, time = 30)),
                           var = c(10,20,30),
                      Test = "GLS",
                      id = "Series Length")

mcc.time <- rbind(mcc_mk_time, mcc_pw_time, mcc_gls_time)


time <- ggplot(data = mcc.time, aes(x = var, y = mcc, group = Test))+
  geom_line(aes(color = Test), size = 1.1) +
  geom_point(aes(color = Test), size = 1.5) +
  scale_x_discrete(limits=c(10,20,30))+
  labs(x = "Series length",
       y = "MCC") +
  theme(axis.text = element_text(colour="grey20",size=13,hjust=.5,vjust=.5,face="plain"),
        axis.title.x = element_text(colour="grey20",size=17,angle=0,vjust=-1,face="plain"),
        axis.title.y = element_text(colour="grey20",size=17,face="plain")) +
  xlim(5,35)

plot_grid(ar, time, align = "h", rel_widths = c(1, 1.51), labels = c("A","B"),
          label_x = c(0.225,0.15),label_fontface = 'plain')
```

