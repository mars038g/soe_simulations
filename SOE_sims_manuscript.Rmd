---
title: "A simulation study of trend detection methods for Integrated Ecosystem Assessment"
output:
  pdf_document: 
  word_document: default
  html_document:
    df_print: paged
indent: yes
geometry: margin=1in
bibliography: SOE simulations.bib
csl: ices-journal-of-marine-science.csl
documentclass: ouparticle
---


```{r setup, include=FALSE}
data.dir <- here::here("data.dir")
r.dir <- here::here("R")

# list of all packages required
packages <- c("stringi","boot","tinytex","Kendall","zoo","zyp",
              "trend","dplyr","AICcmodavg","nlme",
              "gtools","tidyr","stringr","ggplot2",
              "data.table","scales","RColorBrewer",
              "colorspace","mccr","cowplot","gridExtra","grid")

installLoadPackages <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE,repos='http://cran.us.r-project.org')
    sapply(pkg, require, character.only = TRUE)
}

installLoadPackages(packages)

```

```{r functions, echo = F}
#GLS model selection function
source(file.path(r.dir,"fit_lm.R"))

#A funciton to run simulations
source(file.path(r.dir,"sim_function.R"))

#A function to create confusion matrices
source(file.path(r.dir,"conf_mat.R"))

#Simulation parameters
load(file.path(data.dir, "sim_param.Rdata"))

#2017 SOE report data
load(file.path(data.dir,'SOE_data_2017.RData'))

#List of fields from 2017 SOE report
load(file.path(data.dir,"SOE_fields.Rdata"))

#Label facets
label <- function(variable,value){
  return(facet_names[value])
}

#Color palette
pal <- colorRampPalette(c("white","darkred"))

#set seed
set.seed(123)

#Choose to run simulations
run <- F

#Choose to save new data 
save_clean <- F

#Numbers of simulations
nsims <- 5000

# ARsd <- .54^.5 #standard deviation of time series
# ARsd_up95 <- 0.93^.5 #95% CI for variance
# 
# #AR strengths
# NOAR <- list()
# medAR <- 0.433
# strongAR <- 0.8

```




\centerline{Sean Hardison}
\centerline{Northeast Fisheries Science Center, Woods Hole, MA, 02543-1026, USA}
\centerline{sean.hardison@noaa.gov} \  

\centerline{Charles Perretti}
\centerline{Northeast Fisheries Science Center, Woods Hole, MA, 02543-1026, USA}
\centerline{charles.perretti@noaa.gov} \  

\centerline{Geret DePiper}
\centerline{Northeast Fisheries Science Center, Woods Hole, MA, 02543-1026, USA}
\centerline{geret.depiper@noaa.gov} \  

\centerline{Andrew Beet}
\centerline{Northeast Fisheries Science Center, Woods Hole, MA, 02543-1026, USA}
\centerline{andrew.beet@noaa.gov} \  

\linenumbers
## Abstract

The identification of trends in ecosystem indicators has become a core component of ecosystem approaches to resource management, although oftentimes assumptions of statistical models are not properly accounted for in the reporting process. To explore the limitations of trend analysis of short times series, we applied three common methods of trend detection, including a generalized least squares model selection approach, the Mann-Kendall test, and Mann-Kendall test with trend-free pre-whitening to simulated time series of varying trend and autocorrelation strengths. Our results suggest that the ability to detect trends in time series is hampered by the influence of autocorrelated residuals in short series lengths. While it is known that tests designed to account for autocorrelation will approach nominal rejection rates as series lengths increase, the results of this study indicate biased rejection rates in the presence of even weak autocorrelation for series lengths often encountered in indicators developed for ecosystem-level reporting (N = 10, 20, 30). This work has broad implications for ecosystem-level reporting, where indicator time series are often limited in length, maintain a variety of error structures, and typically are assessed using a single statistical method applied uniformly across all time series. If a hypothesis testing approach for indicator trend analysis is to be implemented, we suggest first characterizing candidate series based on suitability (e.g. based on variance, autocorrelation, and series length) rather than a uniform application of tests for trend. A parametric approach to trend assessment could then be used to provide estimates of uncertainty and trend strengths from probability distributions.
\newline
\newline
\textbf{Keywords}: Ecosystem-based fisheries management; trend analysis; ecosystem indicators


\newpage

## Introduction

 The development and analysis of indicators plays a key strategic role in implementing the Ecosystem Approach for a host of science, management, and intergovernmental organizations [e.g. @NOAA2006; @ICES2013; @SecretariatoftheConventiononBiologicalDiversity2004; @Pices2010; @Garcia2003; @Levin2009a]. At least partially in support of this, substantial effort has been invested in assessing indicator status and trends for the purpose of ecosystem reporting, in all of its guises [e.g. @Garfield2016; @NEFSC2018a; @NEFSC2018b; @NEFSC2018c; @NEFSC2018d; @Blanchard2010; O'Brien 2017; @Butchart2010].

Ecosystem-level indicators often vary greatly with respect to the length of the series under investigation. The ultimate goal of providing integrated advice often leads analysts to truncate longer datasets; generating a consistent series length across indicators for comparison purposes [e.g. @Blanchard2010; @Shin2010; @Shannon2010; @Canales2015]. Further reinforcing this approach is the fact that managers tend to focus on short-term issues [@SecretariatoftheConventiononBiologicalDiversity2004; @Wagner2013], which ultimately necessitates the assessment of trajectories at relatively short time scales.

These issues can lead to the use of short time series for the purpose of ecosystem reporting; i.e. less than 20 data points per indicator [@Blanchard2010; @Shin2010; @Shannon2010; @Canales2015; @Mackas2001; @Nicholson2004]. Statistical trend analysis of indicator data is the gold standard for managers, stakeholders, and analysts. However, in reality trend analysis in this context can be extremely difficult. Evidence indicates that the statistical power to identify trends using short time series may be limited in general [@bence1995; @Nicholson2004; @Wagner2013]. The hydrological, climatological, and statistical  literature shows that autocorrelation in time series can falsely inflate trend detection rates when models are incorrectly specified assuming the independence of error terms [@Kulkarni1995; @woodward1997; @hamed1998; @VonStorch1999a; @Nicholls2001a; @roy2004; @Zhang2000; @Wang2001a;  @Yue2002a; @Bayazit2015]. The magnitude of assigned trends can also be inflated by the presence of autocorrelation, and both of these problems are amplified by short time series [@Kulkarni1995; @Yue2002a]. Despite this, there has been no systematic investigation for the performance of models in detecting trends across the full breadth of indicators utilized in ecosystem reporting.

In this manuscript we abstract away from issues surrounding the identification and vetting of appropriate indicators, but note that this in itself can be a challenging undertaking for which @Bundy2017 present a survey of the literature. We focus, instead, on the ability to statistically identify trends for the broad array of indicators used in marine ecosystem reporting; ranging from large-scale climatological and oceanographic drivers through the benefits derived by human society. We use Monte Carlo simulations to assess the performance of the most commonly applied statistical models under a range of time series lengths, trend strengths, and autocorrelation regimes. The simulations are parameterized using the properties of indicators currently presented in the Mid-Atlantic and New England State of the Ecosystem Reports, which are annual ecosystem status reports tailored for the U.S. Mid-Atlantic and New England Fishery Management Councils respectively [@NEFSC2018a; @NEFSC2018b].

Results indicate that correctly identifying trends is problematic using less than 30 data points, with both Type I and Type II error common. Even under the strongest signal-noise ratio (i.e. strong trends and no autocorrelation) tests perform poorly when series lengths are less than 30. The simulations highlight problems associated with standardizing approaches across indicators, and suggest that further thought is warranted on status and trend analysis in the context of ecosystem reporting.

## Methods

#### Data

Parameters used in simulations were chosen based on preliminary analyses characterizing the distribution of trend and autocorrelation strengths across 124 time series that were candidates for inclusion in the 2017 State of the Ecosystem (SOE) reports [@NEFSC2018a; @NEFSC2018b] (Fig. 1). Trends in these candidate time series were characterized by linear regression, with the mean and upper 95% confidence interval for slopes chosen for representation in simulations. The $\rho$ components of SOE time series were estimated by maximum likelihood estimation (MLE), and the distribution mean was chosen as our "medium autocorrelation" parameter for simulated series. To reasonably parameterize simulation innovation variance, we fit all residual series with an AR(1) model estimating innovation variance using MLE, and then found the mean of the resulting distribution of variances.

```{r preliminary analyses, echo = F, fig.align="center", fig.asp=0.3, fig.cap="Frequency of estimated slopes (A), autocorrelation strengths (B), and time series variances (C) in data considered for inclusion in the 2017 State of the Ecosystem report. The solid red lines (A-C) represent distribution means, and dashed red lines (A, C) show the upper 95\\% confidence intervals for estimated trend slope and variance. \\label{Fig1}", warning=FALSE}

#A function to *approximate* starting simulation parameters with linear models------------------------
nlm <- function(field, lin = NULL, norm = NULL){
  time <- SOE.data[SOE.data$Var == field,]$Time
  end = max(time)
  time = time[1:which(time == end)]
  
  var <- SOE.data[SOE.data$Var == field,]$Value
  var <- var[1:length(time)]
  
  
  if(norm == TRUE){
    var = (var-mean(var))/sd(var)
  } else {
    var = var
  }
  
  if(lin == TRUE){
    time <- c(1:length(time))
    mod <- lm(var ~ time)
    int <- mod$coefficients[1]
    beta <- mod$coefficients[2]
  } else if (lin == FALSE){
    time <- c(1:length(time))
    time2 <- time^2
    mod <- lm(var ~ time + time2)
    int <- mod$coefficients[1]
    beta <- mod$coefficients[c(2,3)]
  }
  out <- c(int, beta)
  return(out)
}

#A function to estimate AR(p) parameters--------------------------------------------------------------
handle_arima <- function(field, a){
  tryCatch(
    {
      x.ts <- SOE.data %>% 
        filter(Var == field) %>% 
        mutate(Value = (Value-mean(.$Value))/sd(.$Value)) %>% #normalize
        arrange(Time) %>% #arrange
        dplyr::select(Time, Value) 
      
      res <- lm(Value ~ Time, data = x.ts)$residuals
      
      mod <- arima(x = res, order = c(a,0,0))
      
      if (a == 2){
        out <- data.frame(ar1 = mod$coef[1],
                          ar2 = mod$coef[2],
                          var = mod$sigma2,
                          field = field)
      } else {
        out <- data.frame(ar1 = mod$coef[1],
                          ar2 = NA,
                          var = mod$sigma2,
                          field = field)
       
      }
      
      return(out)
    },
    error=function(error_message) {
      message(error_message)
      return(data.frame(ar1 = NA,
                          ar2 = NA,
                          var = NA,
                          field = field))
    }
  )
}

#Estimate AR(1)
ar1.out <- NULL
for (i in fields){
  assign("ar1.out",rbind(handle_arima(i, a = 1), ar1.out))
}

#Estimate AR(2)
ar2.out <- NULL
for (i in fields){
  assign("ar2.out",rbind(handle_arima(i, a = 2), ar2.out))
}

#Estimate linear coefficients
beta_lin <- mapply(nlm, fields, lin = TRUE, norm = TRUE)
lin_coefficients <- data.frame("Fields"  = fields,
                               "Intercept" = beta_lin[1,],
                               "beta1" = beta_lin[2,])

#Data summaries---------------------------------------------------------------------------------------

#regression slopes
abs_beta <- abs(beta_lin[2,]) 
mean_beta1_linear <- mean(abs_beta)
upper_95_beta1 <- quantile(x = abs_beta, probs = 0.95)

#AR(1)
mean_ar1 <- mean(ar1.out$ar1, na.rm = T)
upper_95_ar1 <- quantile(x = ar1.out$ar1, probs = 0.95, na.rm = TRUE)

#Variance
iv_mean_ar1 <- mean(ar1.out$var, na.rm = T)
upper_75_iv_ar1 <- quantile(x = ar1.out$var, probs = 0.75, na.rm = TRUE)
lower_25_iv_ar1 <- quantile(x = ar1.out$var, probs = 0.25, na.rm = TRUE)

#AR(2)
mean_ar2_coef1 <- mean(ar2.out$ar1, na.rm = T)
mean_ar2_coef2 <- mean(ar2.out$ar2, na.rm = T)
iv_mean_ar2 <- mean(ar2.out$var, na.rm = T)

trend.df <- data.frame(Var = c("none","weak","med","strong"),
                       Value = c(0,0.026, 0.051, 0.147))

#Save clean data--------------------------------------------------------------------------------------

if (save_clean){
  
 save(mean_beta1_linear, upper_95_beta1,
     mean_ar1, upper_95_ar1,
     iv_mean_ar1, upper_75_iv_ar1,
     lower_25_iv_ar1,
     mean_ar2_coef1, mean_ar2_coef2,
     iv_mean_ar2, trend.df,
     file = file.path(data.dir,"sim_param.rdata"))

}

#Build plots------------------------------------------------------------------------------------------

beta <- ggplot(data = lin_coefficients, aes(x = abs(beta1))) +
  geom_histogram(bins = 124, binwidth = 0.005,
                 fill = "lightblue", color = "black",
                 position = "dodge") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,16.5)) +
  xlab(expression(alpha[1])) +
  ylab("") +
  geom_vline(aes(xintercept = mean_beta1_linear), col = "indianred", size = 1) +
  geom_vline(aes(xintercept = upper_95_beta1), col = "indianred", size = 1, linetype = "dashed") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))

ar1 <- ggplot(data = ar1.out, aes(x = ar1)) +
  geom_histogram(bins = 124, binwidth = 0.05,
                 fill = "lightblue", color = "black",
                 position = "dodge") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,13)) +
  xlab(expression(rho)) +
  ylab("") +
  geom_vline(aes(xintercept = mean_ar1), col = "indianred", size = 1) +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))  

iv <- ggplot(data = ar1.out, aes(x = var)) +
  geom_histogram(bins = 124, binwidth = 0.05,
                 fill = "lightblue", color = "black",
                 position = "dodge") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,16.5)) +
  xlab("Series Variance") +
  ylab("") +
  geom_vline(aes(xintercept = iv_mean_ar1), col = "indianred", size = 1) +
  geom_vline(aes(xintercept = upper_95_iv_ar1), col = "indianred", size = 1, linetype = "dashed") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))

cowplot::plot_grid(beta, ar1, iv, nrow = 1,
                   labels = c("A","B","C"),
                   label_fontface = "plain",
                   label_x = c(0.125,0.15,0.125))

```

#### Simulations

Simulated time series were generated through the addition of $AR(1)$ autoregressive processes to first-order linear models:

\begin{equation}
\begin{split}
Y_{t} = \alpha_{0} + \alpha_{1}X_{t} + \varepsilon_{t} \\
\varepsilon_{t} = \rho\varepsilon_{t-1} + \omega_{t} \\
\omega_{t} \sim N(0, \sigma^{2})
\end{split}
 \label{eqn:1}
\end{equation}

where $Y_{t}$ is the simulated series at time $t$, $\alpha_{1}$ is the slope component, and $\varepsilon_{t}$ is the AR1 error process; the strength of which is given by $\rho$, with the error component $\omega_{t}$ assumed to be derived from Gaussian white noise. Through the preliminary analysis detailed above, the levels of $\alpha_{1}$ were 0.026, 0.051, and 0.147, which we combined with three levels of $\rho$: 0, 0.43, and 0.8. For each trend strength, autocorrelation strength, and time series length, 1000 simulations were performed. To test the null hypothesis of no trend in simulated time series, we used a generalized least squares (GLS) model selection process, Mann Kendall test, and Mann Kendall test with trend-free pre-whitening. 

We focused our analyses on rejection rates of the null hypothesis of no trend, as this methodology is a common framework for assessing the flexibility of trend models to deviations from assumptions [e.g. @Yue2002a; @Yue2002b]. Further, null hypothesis testing is often applied in ecosystem indicator reporting for assessing trend [e.g. @NEFSC2018a; @NEFSC2018b]. We chose to extend this analysis of rejection rates for the scenario of no trend and strong autocorrelation to larger sample sizes (N = 50-650) to highlight the shortcoming of small sample sizes when strong autocorrelation is present. To ensure the simulated output approached asymptotic error rates, we also increased the number of simulations in this scenario to 1500 runs. Our final analysis compared the efficacy of the non-parametric Sen's slope to the GLS estimator for assigning linear trend to data where trend was found to be significant (P < 0.05). 

#### Generalized least squares

A GLS model selection procedure was implemented to test for trend in simulated series. Two first order linear and two quadratic GLS models were fit to each simulated time series and best models were chosen using AIC corrected for small sample size (AICc). Specifically, the models were 1) linear trend with uncorrelated residuals, 2) linear trend with correlated residuals, 3) quadratic trend with uncorrelated residuals, and 4) quadratic trend with correlated residuals. Component GLS models were derived from


\begin{equation}
\begin{split}
Y_{t} = \alpha_{0} + \alpha_{1}X_{t} + \alpha_{2}X_{t}^{2} + \varepsilon_{t} \\
\varepsilon_{t} = \rho\varepsilon_{t-1} + \omega_{t} \\
\omega_{t} \sim N(0, \sigma^{2}).
\end{split}
 \label{eqn:2}
\end{equation}


The above model follows the same notation as our simulated series. Setting $\alpha_{2} = 0$ yielded linear trend models, and $\rho = 0$ gave models with uncorrelated residuals. 

#### Mann Kendall test

Further tests for trend in simulated time series were performed using the Mann-Kendall test (MK) [@mann1945; @kendall1955] and the more robust Mann-Kendall test with trend-free pre-whitening (MK-TFPW)[@Yue2002b]. The MK test for trend is a nonparametric approach that assumes sample data are independent and identically distributed. Serial correlation within sample data has been found to lead to inflated rejection rates of the null hypothesis of no trend if no correction steps are applied to the MK test [@Kulkarni1995]. Residual pre-whitening is a common correction to address autocorrelation within MK tests, although pre-whitening is known to reduce the magnitude of existing trend [@Yue2002a]. The MK with trend-free pre-whitening is a step-wise procedure developed by @Yue2002b to address issues introduced by pre-whitening, and is further detailed below. Under both MK and MK-TFPW frameworks, Kendall's tau statistic is given by:



\begin{equation}
\begin{split}
S = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\textrm{sgn}(Y_{j} - Y_{i}), \\
\end{split}
 \label{eqn:3}
\end{equation}

where $Y$ is the response vector, $n$ is the length of the series, and

\begin{equation}
\begin{split}
\textrm{sgn}(x)=\begin{Bmatrix}
1 & \textrm{if}\:x>0\\ 
0 & \textrm{if}\:x=0\\ 
-1 & \textrm{if}\:x<0
\end{Bmatrix}. \\
\end{split}
 \label{eqn:4}
\end{equation}


When there are no ties in the data, the variance of $S$ is given by 

\begin{equation}
\begin{split}
V(S) = \frac{n(n-1)(2n+5)}{18}, \\
\end{split}
 \label{eqn:5}
\end{equation}

and the distribution of $S$ when $n \geq 8$ is approximately normal and symmetric about a mean of 0 and variance, V(S). The standardized test statistic, 



\begin{equation}
\begin{split}
\textrm{Z}=\begin{Bmatrix}
\frac{S-1}{\sqrt{V(S)}} & S>0\\ 
0 & S=0\\ 
\frac{S+1}{\sqrt{V(S)}} & S<0\\ 
\end{Bmatrix}, 
\end{split}
 \label{eqn:6}
\end{equation}

is normally distributed with mean of zero and variance of one [@Wang2001a]. The null hypothesis of no trend is rejected at significance level $\alpha$ if the probability $1-\Phi(|Z|) < \alpha$, where $\Phi(x)$ is the standard normal cumulative distribution funciton.

#### Mann-Kendall trend-free pre-whitening
The Mann Kendall trend-free pre-whitening procedure as developed by @Yue2002a is composed of four steps:


\begin{enumerate}
\item \textit{Removal of trend} -  The Theil-Sen estimator (Sen, 1968; Theil, 1992) is used to estimate the slope of trend $b$, which is removed from sample data if different from zero. $b$ is given by


\begin{equation}
\begin{split}
b = \mathrm{Median}\left (\frac{y_{j} - y_{i}}{j - l}\right)\forall l < j .
\end{split}
 \label{eqn:8}
\end{equation}

Trend $b$ is removed from the series by

\begin{equation}
\begin{split}
y_{t}^{'} = y_{t} - bt,
\end{split}
 \label{eqn:9}
\end{equation}

where $y_{t}$ is the original series at time step $t$.
\item \textit{Trend-free pre-whitening} - A pre-whitening step is applied to the detrended series to remove the $AR(1)$ component. First, the lag-$1$ autocorrelation coefficient $\rho_{1}$ is found using 

\begin{equation}
\begin{split}
\rho_{k} = \frac{  \frac{1}{n-k} \sum_{t=1}^{n-k}[y_{t} - E(y_{t})][y_{t+k} - E(y_{t})]}  {\frac{1}{n}\sum_{t=1}^{n}[y_{t} - E(y_{t})]^2},
\end{split}
 \label{eqn:10}
\end{equation}

where $E(y_{t})$ is the mean of the series and $\rho_{k}$ is the lag-$k$ autocorrelation coefficient. Serial correlation is then removed from the detrended series $y_{t}^{'}$ by 

\begin{equation}
\begin{split}
Y_{t}^{'} = y_{t}^{'} - \rho_{1}y_{t}^{'}.
\end{split}
 \label{eqn:11}
\end{equation}

\item \textit{Blending trend and residual series} - Trend $b$ is added to the independent residual series $Y_{t}^{'}$ by 

\begin{equation}
\begin{split}
Y_{t} = Y_{t}^{'} + bt.
\end{split}
 \label{eqn:12}
\end{equation}

\item \textit{MK test} - Trend is assessed through the application of the Mann Kendall test as discussed above.

\end{enumerate}

## Results

```{r assessing model power, echo = F}
if (run){
#AR1
out_ar1 <- NULL
noAR <- 0
for (r in c("noAR","mean_ar1","upper_95_ar1")){
  for (n in c(10,20,30)){
    for (t in c("none","weak","med","strong")){
      for (i in 1:nsims){      
        if (i %in% seq(1,nsims,nsims/20)){
          print(paste("sim =",i,"series length =",n,"trend =",t,"ar =",r))
        }
        
        assign("out_ar1",rbind(out_ar1, test.series(ar.order = 1,
                                                    rho = r,
                                                    series.length = n,
                                                    trend.strength = t,
                                                    var = iv_mean_ar1,
                                                    nsims = nsims))) 
      }
    }
  }
}

#AR1 with 0.95 quantile of variance sample distribution
#75 and 25; include that we repeated for different levels of variance and similar patterns existed etc.
var2_ar1 <- NULL
noAR <- 0
for (r in c("noAR","mean_ar1","upper_95_ar1")){
  for (n in c(10,20,30)){
    for (t in c("none","weak","med","strong")){
      for (v in c("upper_75_iv_ar1","lower_25_iv_ar1")){
        for (i in 1:nsims){      
        if (i %in% seq(1,nsims,nsims/20)){
          print(paste("sim =",i,"series length =",n,"trend =",t,"ar =",r))
        }
        
        assign("var2_ar1",rbind(var2_ar1, test.series(ar.order = 1,
                                                    rho = r,
                                                    series.length = n,
                                                    trend.strength = t,
                                                    var = get(v),
                                                    nsims = nsims))) 
        }
      }
    }
  }
}

#AR2
#Rewrite fit_lm to include AR1, AR2 and normally distributed
out_ar2 <- NULL
for (n in c(10,20,30)){
   for (t in c("none","weak","med","strong")){
    for (i in 1:nsims){      
      if (i %in% seq(1,nsims,nsims/20)){
        print(paste("sim =",i,"series length =",n,"trend =",t))
      }
      
      assign("out_ar2",rbind(out_ar2, test.series(ar.order = 2,
                                                  series.length = n,
                                                  trend.strength = t,
                                                  rho = NA,
                                                  var = iv_mean_ar2,
                                                  nsims = nsims))) 
    }
  }
}

#strong AR and no trend 
null_case <- NULL
for (r in c("upper_95_ar1")){
  for (n in seq(250,650,50)){
    for (t in c("none")){
      for (i in 1:nsims){      
        if (i %in% seq(1,nsims,nsims/20)){
          print(paste("sim =",i,"series length =",n,"trend =",t,"ar =",r))
        }
        
        assign("null_case",rbind(null_case, test.series(ar.order = 1,
                                                    rho = r,
                                                    series.length = n,
                                                    trend.strength = t,
                                                    var = iv_mean_ar1,
                                                    nsims = nsims))) 
      }
    }
  }
}

if (save_clean){
  save(null_case, out_ar1, out_ar2, var2_ar1, file = file.path(data.dir,"simulation_results.Rdata"))
}

} else {
  load(file.path(data.dir, "simulation_results.Rdata"))
}


```

```{r simulation results processing, echo = F}

set_levels <- function(df){
  df$trend <- df %>%
                pull(trend) %>%
                plyr::mapvalues(.,from = c("none","weak","med","strong"),
                                to = c("No trend", "Weak trend","Med. trend", "Strong trend")) %>% 
                factor(., levels=c('Strong trend','Med. trend','Weak trend','No trend'))
  
  if (max(as.numeric(df$series.length))==30){
    df$series.length <- df %>% 
        pull(series.length) %>% 
        factor(.,levels = c(10,20,30))
  } else {
    df$series.length <- df %>% 
        pull(series.length) %>% 
        factor(.,levels = seq(50,650,50))
  }
  
  
  df$test <- df %>% 
    pull(test) %>% 
    plyr::mapvalues(., from = c("gls","mk","pw"), 
                    to = c("GLS","Mann-Kendall","MK-TFPW")) %>% 
    factor(.,levels = c("GLS","Mann-Kendall","MK-TFPW"))
  
  if (is.na(df$rho2[1])){
    df <- df %>%
      mutate(rho1 = round(rho1, 3)) %>% 
      mutate(rho1 = plyr::mapvalues(rho1,from = c(0,0.455,0.9),
                                 to = c("No AR","Med. AR","Strong AR")),
                           rho1)
    if (length(unique(df$rho1)) == 3){
      df$rho1 <- df %>% 
        pull(rho1) %>% 
        factor(., levels = c("No AR","Med. AR", "Strong AR"))
    }
    
  }
  
  df <- df %>% dplyr::rename(Method = test)
  
  return(df)

}

out_ar1_int <- set_levels(out_ar1)
var2_ar1_int <- set_levels(var2_ar1)
out_ar2_int <- set_levels(out_ar2)
null_case_int <- set_levels(null_case)


#Aggregate p values
p_agg <- function(df){
  df %>% 
    group_by(Method, series.length, rho1, trend) %>% 
    dplyr::summarise(prop = length(p[p < 0.05])/n())
}

agg_out_ar1 <- p_agg(out_ar1_int)
agg_var2_ar1 <- p_agg(var2_ar1_int)
agg_out_ar2 <- p_agg(out_ar2_int) #Only one level of rho in AR(2); extra grouping doesn't matter
agg_null_case <- p_agg(null_case_int)


#some values for text
gls <- agg_out_ar1 %>% dplyr::filter(rho1 == "No AR", Method == "GLS")
pw <- agg_out_ar1 %>% filter(rho1 == "No AR", Method == "MK-TFPW")

perc_change1 <- abs(mean(pw$prop) - mean(gls$prop))/mean(gls$prop) * 100
nom_p <- agg_out_ar1 %>% filter(trend == "No trend", rho1 == "No AR", series.length == 10)
strAR_noTR <- agg_out_ar1 %>% filter(trend  == "No trend", rho1 == "Strong AR", series.length == 30)

perc_change_mk <- round(abs(strAR_noTR[strAR_noTR$Method == "Mann-Kendall",]$prop - strAR_noTR[strAR_noTR$Method == "GLS",]$prop)/
                        (strAR_noTR[strAR_noTR$Method == "Mann-Kendall",]$prop) * 100,0)

perc_change_pw <- round(abs(strAR_noTR[strAR_noTR$Method == "MK-TFPW",]$prop - strAR_noTR[strAR_noTR$Method == "GLS",]$prop)/
                        (strAR_noTR[strAR_noTR$Method == "MK-TFPW",]$prop) * 100,0)

#sample size effect
ss1 <- agg_out_ar1 %>% filter(trend  == "No trend", rho1 == "Strong AR", series.length == 10)
ss2 <- agg_out_ar1 %>% filter(trend  == "No trend", rho1 == "Strong AR",  series.length == 30)

pw1 <- ss1[ss1$Method == "MK-TFPW",]$prop
pw2 <- ss2[ss2$Method == "MK-TFPW",]$prop

pw_ss <- round((pw1 - pw2)/pw1, 2)*100

gls1 <- ss1[ss1$Method == "gls",]$prop
gls2 <- ss2[ss2$Method == "gls",]$prop

gls_ss <- round((gls1 - gls2)/gls1, 2)*100

```

Throughout this study we adopt an alpha value of 0.05 to assess significance of statistical results. Overall, no method performed consistently well in all scenarios of simulated trend strength, time series length, and autocorrelation strength. As has been documented elsewhere (Yue and Wang 2002; Yue et al. 2002), we find time series length has a large effect on the power of each test (Figure \ref{Fig2}), and performance was generally best across autocorrelation and trend scenarios when N = 30. With no autocorrelation and trend present, trends were only detected with > 90% accuracy when trend was strong ($\alpha_{1} = 0.147$). Even with a strong trend and no autocorrelation, no test detected a trend in greater than 50% of the series when N = 10. Again under no autocorrelation, the increased power associated with increasing series length diminished with reductions in trend strength across all tests. The GLS test showed the highest rejection rates compared to other tests under no autocorrelation, although this effect was minimal (percent increase of rejection rates between GLS and MK-TFPW  was ~`r round(perc_change1,0)`%). All tests returned rejection rates near the nominal significance level of 0.05 under the no trend and no autocorrelation scenarios, with the largest departures occurring when N = 10 (MK-TFPW~sig~ = `r round(nom_p[nom_p$test == "pw",]$prop,3)`, MK~sig~ = `r round(nom_p[nom_p$test == "mk",]$prop,3)`, GLS~sig~ = `r round(nom_p[nom_p$test == "gls",]$prop,3)`).
  
```{r power analysis figure, echo = F, fig.align='center',fig.cap="Barplots showing the propotion of significant trends (\\textit{P}<0.05) to number of total simulations. Subplots are representative of different autocorrelation ($\\rho = 0, .43, .8$) and trend scenarios ($\\alpha_{1} = 0.026, .051$), with time series length increasing along the x axis. Colored bars show results from different tests for trend. \\label{Fig2}"}

ggplot(agg_out_ar1,aes(color = Method, y = prop, 
                  x = series.length)) +
  geom_bar(aes(fill = Method), stat = "identity",  position="dodge",
            size = 0.5, color = "black") +
  facet_grid(trend ~ rho1, labeller = labeller(tbl)) +
  ylab("Proportion significant") +
  xlab("Series length") +
  scale_fill_brewer(palette = "PuBu", direction = -1) +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))

```

  Autocorrelation is known the reduce the power of the MK test by increasing the variance of the \textit{S} statistic [@VonStorch1999a; @Yue2002a], and our work also shows that under no simulated trend, introducing autocorrelation will lead to inflated rejection rates in the MK test. The bottom row of Figure \ref{Fig2} shows that under no trend and medium to strong autocorrelation ($\rho = 0.433$ and $\rho = 0.8$), the rejection rate of the Mann Kendall test increases with series length. All other tests showed decreases in rejection rates.

  The GLS procedure performed the best under the no trend and strong autocorrelation scenario: when N = 30, the rejection rate for the GLS was `r strAR_noTR[strAR_noTR$test == "gls",]$prop`; `r perc_change_pw`% and `r perc_change_mk`% lower than the MK-TFPW and MK tests respectively. The performance of the GLS test was also more strongly affected by sample size than the MK-TFPW test. When there was strong autocorrelation and no trend, rejection rates of the MK-TFPW test decreased only `r pw_ss`% between N = 10 and N = 30. Under the same conditions and GLS approach, rejection rates decreased by `r gls_ss`%. However, the GLS approach also performed the worst under no trend and strong autocorrelation when N = 10. This shows that while there was improvement between both tests as series lengths increased, neither test was effective in accounting for biases of autocorrelation when $N \leq 30$.

Extending this no trend and strong autocorrelation scenario out to longer series lengths shows that the GLS test reaches nominal rejection rates of 0.05 only when $N \geq 500$ (Figure \ref{Fig3}). The MK-TFPW approach performed poorly in this analysis, and did not converge to nominal rejection rates for $N < 500$, although this work did not seek to identify a precise value of N where the MK-TFPW approach reached nominal levels. As expected, the MK test performed poorly in this scenario, and saw no reduction in rejection rates as N increased. 

```{r power subset figure, fig.align="center", fig.asp=0.45, eval = T, echo = F, fig.show = "h", fig.cap= "Barplot showing the ratio of number of rejections (\\textit{P}<0.05) to number of total simulations, when simulations were created under the parameters of no trend $(\\alpha_{1} = 0)$, strong autocorrelation $(\\rho = 0.8)$, and series lengths between \\textit{N} = 50 to $N = 500$. The dashed red line shows the nominal rejection rate of 0.05.\\label{Fig3}"}


agg_null_case %>% 
  filter(trend == "No trend") %>% 
ggplot(aes(x = series.length,
           y = prop,
           group = Method,
           fill = Method)) +
  geom_bar(stat = "identity",position="dodge", color = "black") +
  geom_hline(yintercept=0.05, linetype="dashed", 
             color = "red", size=1) +
  ylab("Proportion significant") +
  xlab("Series length") +
  scale_fill_brewer(palette = "PuBu", direction = -1) +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))
```

Shown in Figure \ref{Fig2} under strong autocorrelation ($\rho = 0.8$) and strong trend ($\alpha_{1} = 0.147$), the relationship between time series length and rejection rate was positive, highlighting the importance of the trend signal strength on test results. Under these parameters, the MK-TFPW test performed slightly better than the GLS approach, although both tests were only able to detect trend in >50% of simulations when trend was strong and N = 20 or N = 30. When $\alpha_{1} < 0.0147$ and autocorrelation was strong, neither the GLS nor MK-TFPW tests were able to detect trend in >50% of simulations regardless of series length. Interestingly, as series lengths increased when trend was weak (i.e. $\alpha_{1} = 0.026$) and $\rho = 0.8$, rejection rates tended to decrease for GLS and MK-TFPW tests. The relative success of each test when N = 30 can be seen in Figure \ref{Fig4}, which shows that the GLS approach was most effective in avoiding false positives, but performed similarly to the MK-TFPW test in terms of false negatives.      
  
```{r confusion matrices, echo = F, fig.align='center', out.extra='trim={0cm 5cm 0cm 0cm},clip', fig.cap= 'Confusion matrices showing aggregate results from testing for trend across all combinations autocorrelation and trend strength when N=30. Colors represent the performance of individual cells across tests, where cells shaded in red indicate a poorer outcome. For example, when N=30, the GLS procedure falsely predicted a trend when there was none in 11.1\\% of cases (white), whereas this was true in 22.1\\% of Mann-Kendall simulations (red).\\label{Fig4}' }

mk <- cbind(conf_mat(sim_results,test = "mk", filt = 30), test = rep('mk',4))
pw <- cbind(conf_mat(sim_results, test = "pw", filt = 30), test = rep('pw',4))
gls <- cbind(conf_mat(sim_results, test = "gls", filt = 30), test = rep('gls',4))
fin <- rbind(mk, pw, gls)
fin$group <- factor(paste(fin$x,fin$y))


#Make matrices for white = good and orange = bad
fin_dif <- fin %>% group_by(group) %>%
  mutate(val, best_dif = ifelse(group == "actual no predicted no"|
                                  group == "actual yes predicted yes",
                                (abs(max(val) - val)), #best_dif is for assigning colors
                                (abs(min(val) - val)))) 

#Facet titles
facet_names <- list(
  'mk'="Mann-Kendall",
  'pw'="MK-TFPW",
  'gls'="GLS"
)

#plot
ggplot(data = fin_dif, aes(x,y, fill = best_dif)) +
  facet_grid(. ~ test, labeller = label)+
  geom_tile(aes(size = 1),color = "grey", size = 1)  +
  scale_fill_gradientn(colors = pal(10))+
  geom_text(aes(x = x, y = y, label = round(val,3), size = 1),size = 4) +
  theme(legend.position = "none",
        axis.line = element_blank(),
        axis.title=element_blank(),
        axis.text.y = element_text(margin = margin(t = 0, r = -6,
                                                   b = 0, l = 0),
                                   size = 8),
        axis.text.x = element_text(margin = margin(t = -3, r = 0,
                                                   b = 150, l = 0),
                                   size = 8),
        axis.ticks.y=element_blank(),
        axis.ticks.x=element_blank(),
        plot.title = element_text(hjust = -0.1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
```

We next assessed the ability of each statistical approach to estimate the true trend (Figures \ref{Fig5} and \ref{Fig6}). In the nonparametric case, we used Sen's slope (as derived in Equation 8), which is a common statistic estimated alongside the MK and MK-TFPW significance tests. Sen's slope and the GLS estimator perform similarly across all scenarios. For both methods, the spread of estimated trends increased with autocorrelation strength, although this effect was mediated by increasing series length (e.g. Fig \ref{Fig6}). Further, trends falsely assigned in the "no trend" scenarios tended to have the largest spread. As shown by the black median lines in Figure \ref{Fig5}, both GLS and Sen's slope methods consistently overestimated trend slope when it existed, although both performed well under the strong autocorrelation scenario when trend was strong and N = 30.

```{r trend slope, fig.align="center", echo = F, fig.cap="Violin plots showing probability densities of estimated trends from GLS and Sen's slope procedures under varying autocorrelation scenarios $(\\rho = 0, 0.43, 0.8)$ and simulation lengths (N = 10, 20, 30). Black lines represent the median slope estimate, and red lines the true slope. For this exercise, the GLS model selection procedure was constrained to fit only linear models of trend.\\label{Fig5}"}

load(file.path(data.dir,"sim_results2018-09-18.Rdata"))

df <- sim_results %>% filter(!is.na(slope_pred), p < 0.05) 

#set factor levels for plotting
df$trend <- df %>% pull(trend) %>%
  plyr::mapvalues(., from = c("notrend","ltrendweak","ltrendmed","ltrendstrong"),
                  to = c("no trend","weak trend","medium trend","strong trend")) %>%
  as.factor()

df$ar <- df %>% pull(ar) %>%
  plyr::mapvalues(., from = c("NOAR","medAR","strongAR"),
                  to = c("no AR","medium AR","strong AR")) %>%
  as.factor()

df$trend = factor(df$trend, levels=c('strong trend','medium trend','weak trend','no trend'))
df$ar = factor(df$ar, levels=c('no AR','medium AR','strong AR'))
df$series.length = factor(df$series.length, levels=c(10,20,30))
names(df)[1] <- "Method"
levels(df$Method) <- c("GLS","Mann-Kendall","Sen's slope")

ggplot(df, aes(factor(series.length), slope_pred)) + 
  geom_hline(aes(yintercept = slope_true), color = "red", size = 0.5) +
  geom_violin(aes(fill = Method), adjust = 0.5, scale = "width",
              draw_quantiles = 0.5, size = 0.3) +
  facet_grid(trend ~ ar) +
  ylab("Predicted slope") +
  xlab("Series length") +
  scale_fill_brewer(palette = "PuBu") +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))
```


```{r sd of trend estimates, fig.align = "center", echo = F, fig.cap = "Sample standard deviations of trend estimates derived from Sen's slope and GLS methods across autocorrelation strengths ($\\rho = 0, 0.433, 0.8$), trend strengths ($\\alpha_{1} = 0, 0.026, 0.052, 0.147$), and trend detection methods. \\label{Fig6}"}
stdev <- NULL

for (t in c("GLS","Sen's slope")){
  for (k in c(10,20,30)){
    for (j in c('no trend','weak trend','medium trend','strong trend')){
      for (i in c("no AR","medium AR","strong AR")){
        out <- sd(df[df$Method == t &df$series.length == k &
                       df$trend == j & df$ar == i, ]$slope_pred)
        
        out <- data.frame(stdev = out,
                          Autocorrelation = i,
                          `Trend strength` = j,
                          `Series length` = k,
                          `Method` = t)
        assign('stdev',rbind(stdev,out))
      }
      
    }
    
  }
  
}
names(stdev)[c(3,4)] <- c("Trend strength","Series length")

stdev <- stdev %>% mutate(Autocorrelation, Autocorrelation = plyr::mapvalues(Autocorrelation,
                                                                    from = c("no AR",
                                                                             "medium AR",
                                                                             "strong AR"),
                                                                    to = c(0, 0.433, 0.8))) %>%
  mutate(`Trend strength`, `Trend strength` = plyr::mapvalues(`Trend strength`,
                                                                    from = c("no trend",
                                                                             "weak trend",
                                                                             "medium trend",
                                                                             "strong trend"),
                                                                    to = c("No trend",
                                                                             "Weak trend",
                                                                             "Medium trend",
                                                                             "Strong trend")))



ggplot(data = stdev) + geom_line(aes(x = Autocorrelation, y = stdev,
                                     group = `Trend strength`, color = `Trend strength`),
                                 size = 1) +
  # geom_point(aes(x = Autocorrelation, y = stdev,
  #                                    group = `Trend strength`),
  #            color = "black",
  #                                size = 1)+
  facet_grid(`Method` ~ `Series length`)+
  ylab("Standard deviation of trend estimates") +
  xlab(expression(paste("Autocorrelation (",rho,")"))) +
  scale_color_brewer(palette = "Spectral", direction = -1) +
  theme_bw() +
  theme(plot.title = element_blank(),
        strip.background = element_blank(),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        strip.text = element_text(size = 10))

```


## Discussion
Ecosystem reporting is vital to the development of Integrated Ecosystem Assessments (IEA), which lay out the framework for moving toward Ecosystem-Based Fishery Management (EBFM)[@Levin2009a]. The key analytical foundations to all IEA products revolve around the concept of indicator change; with managers most interested in short-term, abrupt changes to indicator status [@Wagner2013]. Here we addressed the shortcomings of assigning significant trends to indicator time series given the common problems of small sample size and autocorrelation. Our results show that statistical methods commonly used to detect trend in the presence of autocorrelated residuals may be misleading when applied to short time series.

In the Northeast US, indicators considered in the ecosystem assessment process are annual data typically ranging between 10-60 years in length. In the context of hydrological literature, the upper limit of time series lengths seen in our indicator data sets would be considered short [@Bayazit2015]. This study highlights the dangers of assigning trends to short time series even while attempting to address problems of autocorrelation. The influence of autocorrelation in short series inevitably increases Type II error rates (the failure to identify trend when it exists). This was especially true under scenarios of strong autocorrelation, which effectively masked the detection of trend unless trend was strong and N was large. Possibly even more problematic, an increase in Type I error, or the false rejection of the null hypothesis, occurred as the strength of autocorrelation increased. A departure from nominal rejection rates was seen from all tests with even moderate amounts of autocorrelation.

Our work focused on the capacity of tests to detect weak trend strengths in simulated time series. We found that under the limitations imposed by series lengths $\leq$ 30, no test was effective in detecting weak trends, even without autocorrelation (e.g. Figure \ref{Fig2}; no AR and weak trend scenario). Decreasing rejection rates with increasing series lengths for GLS and MK-TFPW tests under weak trend and strong autocorrelation further illustrate the "masking" effect of autocorrelation on trend detection. This result supports the work of others [e.g. @Wagner2013] that have found small trend changes in short time series difficult to detect regardless of autocorrelation. The role of autocorrelation in making trend can in part can be explained by the variance of the random error used in our simulations ($\sigma^{2} = 0.54$). While this may be considered large relative to the our definition of weak trend, it was selected based on the properties of real ecosystem indicators.

As shown in Figure \ref{Fig2}, there is no solution in small sample sizes; however, we do not suggest there is no value in assigning trends to time series. Instead, we advise that a "shotgun" approach to assessing trends in many indicator time series without consideration of error structure and series lengths will likely lead to both Type I and Type II error. If the binary approach of hypothesis testing is to be implemented in ecosystem indicator reporting, a more hands-on approach should be implemented to determine indicators that are well-suited for trend analysis (e.g. a long series with low variance and weak autocorrelation). Further, the results of this study showed that the GLS approach to modeling trend ameliorated error rates compared to the Mann-Kendall test with trend-free pre-whitening. The parametric approach also carries the benefits of modeling trend uncertainty and coefficients, given the assumed probability distribution.

A more intuitive and flexible approach to trend assessment would be to simply present more information with each assessed time series. Nicholls (2001) suggested that the arbitrary (i.e. "p < 0.05") null hypothesis testing framework be replaced by the presentation of confidence intervals for trend effect size. This approach has the potential to provide more contextual information to managers, but as we show above, is limited by the reality that trends (and therefore confidence intervals for effect size) are often misrepresented when series length is small and autocorrelation exists. Supplementing ecosystem reporting documents with methodological summaries could be useful to highlight these limitations and provide realistic expectations for managers [@Wagner2013]. 

A different approach to trend assessment departs from null hypothesis testing altogether in favor of a Bayesian framework. Wagner et al. (2013) suggests Dynamic Linear Models (DLMs) for indicators of small sample size. DLMs allow for model coefficients (e.g. slope) to change with time while providing probabilities of rate changes. This approach introduces greater complexity into the common "up or down" model subscribed to by current ecosystem status reports, and could therefore provide greater insight to managers. In an example of Bayesian regression, Wade (2000) showed how a series with larger variance but a biologically significant trend would be considered non-significant by a frequentist approach, but properly assessed by Bayesian methods. This framework could be adopted by analysts to answer specific questions that resource managers are interested in addressing; e.g. what is the probability that indicator X declined by Y% between this year and last? While Bayesian methods cannot side-step the reality of small sample sizes, their use provides managers with a probabilistic framework for decision-making that is lacking in the frequentist approach [@wade2000; @Wagner2013]. 

Deriving trends from disparate ecosystem indicators is challenging in part due to the goal of applying a single statistical approach to time series with a wide range of series lengths and error structures. The complexity of the chosen method must be balanced with its applicability to a wide range of indicators and the interpretability of its results. Our work shows that blindly implementing this approach will likely result in assigning spurious trends or missing important patterns. However, programmatic consideration of candidate series for trend analysis would likely ameliorate some instance of error. Implementation of a parametric test for trend (e.g. the GLS procedure in our study) then has the benefit of providing estimates of uncertainty and trend based on a probability distribution. A subtler approach for trend analyses in ecosystem reporting would provide better outcomes for economic, ecological, and social systems in the context of EBFM decision-making.

<!--
Although we focused solely on trend detection in indicator time series, our results imply that the limitations inherent within short time series be taken into account during the indicator selection stage. 
--> 
 



```{r decile coverage simulations, echo = F, eval = F}

#placeholders for results
gls.ts.NOAR.notrend <- NULL
gls.ts.NOAR.ltrendweak <- NULL
gls.ts.NOAR.ltrendmed <- NULL
gls.ts.NOAR.ltrendstrong <- NULL

gls.ts.medAR.notrend <- NULL
gls.ts.medAR.ltrendweak <- NULL
gls.ts.medAR.ltrendmed <- NULL
gls.ts.medAR.ltrendstrong <- NULL

gls.ts.strongAR.notrend <- NULL
gls.ts.strongAR.ltrendweak <- NULL
gls.ts.strongAR.ltrendmed <- NULL
gls.ts.strongAR.ltrendstrong <- NULL

sim_results_10 <- NULL
sim_results_20 <- NULL
sim_results_30 <- NULL


if (run){
  ptm <- proc.time()
  #Specify time series length
  for (m in c(10,20,30)){
  
  notrend <- rep(0,m)
  ltrendweak <- -0.262 + (0.004 * c(1:m)) 
  ltrendmed <- -0.262 + (0.051 * c(1:m)) 
  ltrendstrong <- -0.262 + (0.147 * c(1:m)) 
  print(paste("m=",m))

    #Trend strength
    for (k in c("notrend","ltrendweak","ltrendmed","ltrendstrong")){
    
    #AR strength
    for (j in c("strongAR","medAR","NOAR")){
    
      true_trend <- get(k)
      
      for (i in 1:nsims){
        
        #generate simulations
        dat <- arima.sim(list(ar = get(j)), n=m, rand.gen=rnorm, sd = ARsd)
        
        #add autocorrelated error structure to trend
        dat <- get(k) + dat
        dat <- data.frame(series = dat,
                          time = 1:length(dat))
        
        #---------------------------------GLS---------------------------------#
        gls_sim <- tryCatch({
          newtime <- seq(1, m, 1)
          newdata <- data.frame(time = newtime,
                                time2 = newtime^2)
          
          #Correctly specifies model when no AR error in simulated time series
          if (j == "NOAR"){
            gls_sim <- fit_lm(dat = dat, ar = get(j),
                              ARsd = ARsd, m = m, trend = get(k), spec = TRUE)
          } else{
            gls_sim <- fit_lm(dat = dat, ar = get(j), ARsd = ARsd, m = m, trend = get(k))
            
          }
          
        }, 
        error = function(e) {
          gls_sim <- "error"
        })
        
        
        #---------------------------------Decile coverage---------------------------------#
        if (is.na(gls_sim[1]) | gls_sim[1] == "error"){
          
          gls_pred <- data.frame(fit = rep(NA, m))
          decile_of_true <- rep(NA, m)
        } else {
          gls_pred <- AICcmodavg::predictSE(gls_sim$model,
                                            newdata = newdata,
                                            se.fit = TRUE)
          decile_of_true <- ceiling(10 * pnorm(q    = true_trend, 
                                           mean = gls_pred$fit, 
                                           sd   = gls_pred$se.fit))
        }
        
        

        
        for (g in 1:m){
          if (is.na(gls_pred$fit[1])){
            print("NA")
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),NA))
          } else {
            assign(paste0("gls.ts.",j,".",k),rbind(get(paste0("gls.ts.",j,".",k)),decile_of_true[g]))
          }
          
        }
          
        
      } 
      
    }
  }
  sim_results = data.frame(gls.NOAR.ltrendweak = gls.ts.NOAR.ltrendweak,
                           gls.NOAR.ltrendmed = gls.ts.NOAR.ltrendmed,
                           gls.NOAR.ltrendstrong = gls.ts.NOAR.ltrendstrong,
                           gls.NOAR.notrend = gls.ts.NOAR.notrend,

                           gls.medAR.ltrendweak = gls.ts.medAR.ltrendweak,
                           gls.medAR.ltrendmed = gls.ts.medAR.ltrendmed,
                           gls.medAR.ltrendstrong = gls.ts.medAR.ltrendstrong,
                           gls.medAR.notrend = gls.ts.medAR.notrend,

                           gls.strongAR.ltrendweak = gls.ts.strongAR.ltrendweak,
                           gls.strongAR.ltrendmed = gls.ts.strongAR.ltrendmed,
                           gls.strongAR.ltrendstrong = gls.ts.strongAR.ltrendstrong,
                           gls.strongAR.notrend = gls.ts.strongAR.notrend)

    assign(paste0('sim_results_',m), rbind(get(paste0('sim_results_',m)),sim_results))
    #write.csv(sim_results, file = paste0("decile_coverage_results_",m,Sys.Date(),".csv"))
    
    gls.ts.NOAR.notrend <- NULL
    gls.ts.NOAR.ltrendweak <- NULL
    gls.ts.NOAR.ltrendmed <- NULL
    gls.ts.NOAR.ltrendstrong <- NULL
    
    gls.ts.medAR.notrend <- NULL
    gls.ts.medAR.ltrendweak <- NULL
    gls.ts.medAR.ltrendmed <- NULL
    gls.ts.medAR.ltrendstrong <- NULL
    
    gls.ts.strongAR.notrend <- NULL
    gls.ts.strongAR.ltrendweak <- NULL
    gls.ts.strongAR.ltrendmed <- NULL
    gls.ts.strongAR.ltrendstrong <- NULL
    print(proc.time() - ptm)
}

coverage_10 <- sim_results_10  
coverage_20 <- sim_results_20  
coverage_30 <- sim_results_30  
  
write.csv(sim_results_10, file = paste0(data.dir,"coverage_10","_",date,".csv"))
write.csv(sim_results_20, file = paste0(data.dir,"coverage_20","_",date,".csv"))
write.csv(sim_results_30, file = paste0(data.dir,"coverage_30","_",date,".csv"))
  
} else if (!run){

  coverage_10 <- read.csv(file = paste0(data.dir,"coverage_10","_",date,".csv"))
  coverage_20 <- read.csv(file = paste0(data.dir,"coverage_20","_",date,".csv"))
  coverage_30 <- read.csv(file = paste0(data.dir,"coverage_30","_",date,".csv"))
}


```

```{r decile coverage plots, echo = F, fig.align='center', eval = F}

#---------------------------Process data--------------------------#
sim_30 <- tidyr::gather(coverage_30, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_30$`Series length` <- "30"
sim_20 <- tidyr::gather(coverage_20, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_20$`Series length` <- "20"
sim_10 <- tidyr::gather(coverage_10, var, value, gls.NOAR.ltrendweak:gls.strongAR.notrend, factor_key=TRUE)
sim_10$`Series length` <- "10"
sims <- rbind(sim_10, sim_20, sim_30)

sim_table <- sims %>% group_by(`Series length`, var,value)  %>%
  filter(value != 0) %>%
  tally() %>%
  group_by(`Series length`) %>% mutate(n = n/(as.numeric(`Series length`)*nsims))#freq. table


#------------------Split out columns for grouping----------------#
col <- do.call(rbind.data.frame, str_split(sim_table$var, '[.]'))
names(col) <- c("Test","AR","Trend Strength")
sim_table$Test <- col$Test
sim_table$AR <- col$AR
sim_table$Trend.Strength <- col$`Trend Strength`

#Set factor levels for ordering facet_grid()
sim_table$Trend.Strength = factor(sim_table$Trend.Strength, levels=c('ltrendstrong','ltrendmed','ltrendweak','notrend'))

sim_table$AR = factor(sim_table$AR, levels=c('NOAR','medAR','strongAR'))

#------------------Make figure---------------------#

ar_names <- c(`NOAR` = "No AR (phi = 0)",
                    `medAR` = "Med. AR (phi = 0.433)",
                    `strongAR` = "Strong AR (phi = 0.8)")

trend_names <- c(`ltrendstrong`="Strong trend",
                 `ltrendweak` = "Weak trend",
                 `notrend` = "No trend",
                 `ltrendmed` = "Med. trend")
#... + facet_grid(hospital ~ ., labeller = as_labeller(hospital_names))

ggplot(sim_table, aes(color = `Series length`,x = value, y = n)) +
  geom_bar(aes(fill = `Series length`),stat = "identity", position = "dodge", 
           size = 0.1, color = "black") +
  ylab("Fraction of Total Observations in Decile") +
  xlab("Decile") +
  scale_x_discrete(limits = c(1:10)) +
  facet_grid(Trend.Strength ~ AR,  labeller = labeller(Trend.Strength = as_labeller(trend_names),
                                                AR = as_labeller(ar_names))) +
  theme_bw()

```

\newpage

## Acknowledgements 

We would like to thank the members of the State of the Ecosystem Synthesis Working Groups and members of the ICES Working Group on the Northwest Atlantic Regional Sea (WGNARS) for providing valuable feedback during the preliminary phases of writing this manuscript. Support for this work was provided by the NOAA Integrated Ecosystem Assessment Program.

## References